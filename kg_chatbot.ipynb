{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e547e46",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ C√†i ƒê·∫∑t & Import Th∆∞ Vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6369cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Import th√†nh c√¥ng\n"
     ]
    }
   ],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "# !pip install transformers torch sentence-transformers faiss-cpu networkx pandas numpy gradio\n",
    "# !pip install langchain langchain-community\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Import th√†nh c√¥ng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1405947",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ X√¢y D·ª±ng Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07cfc04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Building Knowledge Graph...\n",
      "‚úì Graph built: 2172 nodes, 60607 edges\n",
      "‚úì Indexes created\n",
      "‚úì Graph built: 2172 nodes, 60607 edges\n",
      "‚úì Indexes created\n"
     ]
    }
   ],
   "source": [
    "class KnowledgeGraph:\n",
    "    \"\"\"Knowledge Graph for Alumni Network\"\"\"\n",
    "    \n",
    "    def __init__(self, nodes_file: str, edges_file: str):\n",
    "        self.G = nx.DiGraph()\n",
    "        self.nodes_df = pd.read_csv(nodes_file)\n",
    "        self.edges_df = pd.read_csv(edges_file)\n",
    "        self._build_graph()\n",
    "        self._create_indexes()\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build NetworkX graph from CSV files\"\"\"\n",
    "        print(\"[+] Building Knowledge Graph...\")\n",
    "        \n",
    "        # Add nodes with attributes\n",
    "        for _, row in self.nodes_df.iterrows():\n",
    "            self.G.add_node(\n",
    "                row['id'],\n",
    "                title=row['title'],\n",
    "                node_type=row['type']\n",
    "            )\n",
    "        \n",
    "        # Add edges with relation types\n",
    "        for _, row in self.edges_df.iterrows():\n",
    "            self.G.add_edge(\n",
    "                row['from'],\n",
    "                row['to'],\n",
    "                relation=row['type']\n",
    "            )\n",
    "        \n",
    "        print(f\"‚úì Graph built: {self.G.number_of_nodes()} nodes, {self.G.number_of_edges()} edges\")\n",
    "    \n",
    "    def _create_indexes(self):\n",
    "        \"\"\"Create indexes for fast lookup\"\"\"\n",
    "        self.node_to_title = {node: data['title'] for node, data in self.G.nodes(data=True)}\n",
    "        self.title_to_node = {data['title']: node for node, data in self.G.nodes(data=True)}\n",
    "        self.node_types = {node: data['node_type'] for node, data in self.G.nodes(data=True)}\n",
    "        print(\"‚úì Indexes created\")\n",
    "    \n",
    "    def get_node_info(self, node_id: str) -> Dict:\n",
    "        \"\"\"Get detailed information about a node\"\"\"\n",
    "        if node_id not in self.G:\n",
    "            return None\n",
    "        \n",
    "        node_data = self.G.nodes[node_id]\n",
    "        neighbors_out = list(self.G.successors(node_id))\n",
    "        neighbors_in = list(self.G.predecessors(node_id))\n",
    "        \n",
    "        return {\n",
    "            'id': node_id,\n",
    "            'title': node_data['title'],\n",
    "            'type': node_data['node_type'],\n",
    "            'out_degree': len(neighbors_out),\n",
    "            'in_degree': len(neighbors_in),\n",
    "            'neighbors_out': neighbors_out[:10],  # Limit for display\n",
    "            'neighbors_in': neighbors_in[:10]\n",
    "        }\n",
    "    \n",
    "    def find_path(self, source: str, target: str, max_hops: int = 3) -> List[List[str]]:\n",
    "        \"\"\"Find all paths between two nodes (Multi-hop)\"\"\"\n",
    "        try:\n",
    "            # Find all simple paths up to max_hops\n",
    "            paths = list(nx.all_simple_paths(\n",
    "                self.G, \n",
    "                source, \n",
    "                target, \n",
    "                cutoff=max_hops\n",
    "            ))\n",
    "            return paths\n",
    "        except (nx.NodeNotFound, nx.NetworkXNoPath):\n",
    "            return []\n",
    "    \n",
    "    def get_neighbors_by_relation(self, node_id: str, relation_type: str = None) -> List[Dict]:\n",
    "        \"\"\"Get neighbors filtered by relation type\"\"\"\n",
    "        neighbors = []\n",
    "        \n",
    "        for neighbor in self.G.successors(node_id):\n",
    "            edge_data = self.G[node_id][neighbor]\n",
    "            if relation_type is None or edge_data['relation'] == relation_type:\n",
    "                neighbors.append({\n",
    "                    'node_id': neighbor,\n",
    "                    'title': self.node_to_title[neighbor],\n",
    "                    'relation': edge_data['relation']\n",
    "                })\n",
    "        \n",
    "        return neighbors\n",
    "    \n",
    "    def get_person_careers(self, person: str) -> List[str]:\n",
    "        \"\"\"Get all careers for a person\"\"\"\n",
    "        node_id = self.title_to_node.get(person)\n",
    "        if not node_id:\n",
    "            return []\n",
    "        \n",
    "        careers = []\n",
    "        for neighbor in self.get_neighbors_by_relation(node_id, 'has_career'):\n",
    "            career_title = neighbor['title'].replace('career_', '')\n",
    "            careers.append(career_title)\n",
    "        \n",
    "        return careers\n",
    "    \n",
    "    def search_nodes(self, query: str, node_type: str = None, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search nodes by title\"\"\"\n",
    "        query = query.lower()\n",
    "        results = []\n",
    "        \n",
    "        for node, data in self.G.nodes(data=True):\n",
    "            if query in data['title'].lower():\n",
    "                if node_type is None or data['node_type'] == node_type:\n",
    "                    results.append({\n",
    "                        'node_id': node,\n",
    "                        'title': data['title'],\n",
    "                        'type': data['node_type']\n",
    "                    })\n",
    "                    \n",
    "                    if len(results) >= limit:\n",
    "                        break\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Load Knowledge Graph\n",
    "kg = KnowledgeGraph(\n",
    "    nodes_file='graph_out/nodes_unified.csv',\n",
    "    edges_file='graph_out/edges_unified.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5e40b",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Multi-Hop Reasoning Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efdad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Multi-hop Reasoner initialized\n"
     ]
    }
   ],
   "source": [
    "class MultiHopReasoner:\n",
    "    \"\"\"Multi-hop reasoning on Knowledge Graph\"\"\"\n",
    "    \n",
    "    def __init__(self, kg: KnowledgeGraph):\n",
    "        self.kg = kg\n",
    "    \n",
    "    def check_connection(self, entity1: str, entity2: str, max_hops: int = 3) -> Dict:\n",
    "        \"\"\"Check if two entities are connected\"\"\"\n",
    "        # Find nodes\n",
    "        node1 = self.kg.title_to_node.get(entity1)\n",
    "        node2 = self.kg.title_to_node.get(entity2)\n",
    "        \n",
    "        if not node1 or not node2:\n",
    "            return {\n",
    "                'connected': False,\n",
    "                'reason': 'One or both entities not found'\n",
    "            }\n",
    "        \n",
    "        # Find paths\n",
    "        paths = self.kg.find_path(node1, node2, max_hops)\n",
    "        \n",
    "        if not paths:\n",
    "            return {\n",
    "                'connected': False,\n",
    "                'reason': f'No path found within {max_hops} hops'\n",
    "            }\n",
    "        \n",
    "        # Get shortest path\n",
    "        shortest_path = min(paths, key=len)\n",
    "        \n",
    "        # Build path description\n",
    "        path_desc = self._describe_path(shortest_path)\n",
    "        \n",
    "        return {\n",
    "            'connected': True,\n",
    "            'hops': len(shortest_path) - 1,\n",
    "            'path': [self.kg.node_to_title[n] for n in shortest_path],\n",
    "            'description': path_desc,\n",
    "            'num_paths': len(paths)\n",
    "        }\n",
    "    \n",
    "    def _describe_path(self, path: List[str]) -> str:\n",
    "        \"\"\"Create human-readable path description\"\"\"\n",
    "        desc_parts = []\n",
    "        \n",
    "        for i in range(len(path) - 1):\n",
    "            node1 = path[i]\n",
    "            node2 = path[i + 1]\n",
    "            \n",
    "            title1 = self.kg.node_to_title[node1]\n",
    "            title2 = self.kg.node_to_title[node2]\n",
    "            relation = self.kg.G[node1][node2]['relation']\n",
    "            \n",
    "            desc_parts.append(f\"{title1} --[{relation}]--> {title2}\")\n",
    "        \n",
    "        return \" ‚Üí \".join(desc_parts)\n",
    "    \n",
    "    def check_same_university(self, person1: str, person2: str) -> Dict:\n",
    "        \"\"\"Check if two people attended the same university\"\"\"\n",
    "        node1 = self.kg.title_to_node.get(person1)\n",
    "        node2 = self.kg.title_to_node.get(person2)\n",
    "        \n",
    "        if not node1 or not node2:\n",
    "            return {'answer': 'Unknown', 'reason': 'Person not found'}\n",
    "        \n",
    "        # Get universities for both\n",
    "        unis1 = set()\n",
    "        for neighbor in self.kg.get_neighbors_by_relation(node1, 'alumni_of'):\n",
    "            unis1.add(neighbor['node_id'])\n",
    "        \n",
    "        unis2 = set()\n",
    "        for neighbor in self.kg.get_neighbors_by_relation(node2, 'alumni_of'):\n",
    "            unis2.add(neighbor['node_id'])\n",
    "        \n",
    "        common_unis = unis1.intersection(unis2)\n",
    "        \n",
    "        if common_unis:\n",
    "            uni_names = [self.kg.node_to_title[u] for u in common_unis]\n",
    "            return {\n",
    "                'answer': 'Yes',\n",
    "                'universities': uni_names,\n",
    "                'explanation': f\"{person1} v√† {person2} c√πng h·ªçc t·∫°i: {', '.join(uni_names)}\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'answer': 'No',\n",
    "                'explanation': f\"{person1} v√† {person2} kh√¥ng h·ªçc chung tr∆∞·ªùng\"\n",
    "            }\n",
    "    \n",
    "    def find_common_connections(self, entity1: str, entity2: str) -> Dict:\n",
    "        \"\"\"Find common connections between two entities\"\"\"\n",
    "        node1 = self.kg.title_to_node.get(entity1)\n",
    "        node2 = self.kg.title_to_node.get(entity2)\n",
    "        \n",
    "        if not node1 or not node2:\n",
    "            return {'common': []}\n",
    "        \n",
    "        # Get neighbors\n",
    "        neighbors1 = set(self.kg.G.successors(node1)) | set(self.kg.G.predecessors(node1))\n",
    "        neighbors2 = set(self.kg.G.successors(node2)) | set(self.kg.G.predecessors(node2))\n",
    "        \n",
    "        common = neighbors1.intersection(neighbors2)\n",
    "        \n",
    "        common_list = []\n",
    "        for node in list(common)[:10]:\n",
    "            common_list.append({\n",
    "                'title': self.kg.node_to_title[node],\n",
    "                'type': self.kg.node_types[node]\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'common': common_list,\n",
    "            'count': len(common)\n",
    "        }\n",
    "\n",
    "# Initialize reasoner\n",
    "reasoner = MultiHopReasoner(kg)\n",
    "print(\"‚úì Multi-hop Reasoner initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d6023",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ RAG Context Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947bc509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GraphRAG Retriever initialized\n"
     ]
    }
   ],
   "source": [
    "class GraphRAGRetriever:\n",
    "    \"\"\"RAG system using Graph structure - Bi·ªÉu di·ªÖn m·∫°ng x√£ h·ªôi d∆∞·ªõi d·∫°ng Knowledge Graph\"\"\"\n",
    "    \n",
    "    def __init__(self, kg: KnowledgeGraph, reasoner: MultiHopReasoner):\n",
    "        self.kg = kg\n",
    "        self.reasoner = reasoner\n",
    "        print(\"‚úì GraphRAG Retriever initialized - H·ªá th·ªëng truy xu·∫•t d·ª±a tr√™n ƒë·ªì th·ªã tri th·ª©c\")\n",
    "    \n",
    "    def retrieve_context(self, query: str, max_nodes: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Truy xu·∫•t ng·ªØ c·∫£nh t·ª´ Knowledge Graph d·ª±a tr√™n c√¢u h·ªèi\n",
    "        √Åp d·ª•ng k·ªπ thu·∫≠t GraphRAG: Graph-based Retrieval Augmented Generation\n",
    "        \"\"\"\n",
    "        # Tr√≠ch xu·∫•t entities t·ª´ query\n",
    "        entities = self._extract_entities(query)\n",
    "        \n",
    "        if not entities:\n",
    "            return \"Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong ƒë·ªì th·ªã tri th·ª©c.\"\n",
    "        \n",
    "        # Build context t·ª´ graph structure\n",
    "        context_parts = []\n",
    "        context_parts.append(\"=== TH√îNG TIN T·ª™ KNOWLEDGE GRAPH ===\\n\")\n",
    "        \n",
    "        for entity in entities[:max_nodes]:\n",
    "            node_id = self.kg.title_to_node.get(entity)\n",
    "            if node_id:\n",
    "                # L·∫•y th√¥ng tin node\n",
    "                info = self.kg.get_node_info(node_id)\n",
    "                context_parts.append(self._format_node_context_enhanced(info))\n",
    "                \n",
    "                # L·∫•y th√¥ng tin quan h·ªá (GraphRAG - multi-hop context)\n",
    "                relations_info = self._get_relation_context(node_id)\n",
    "                if relations_info:\n",
    "                    context_parts.append(relations_info)\n",
    "        \n",
    "        # Th√™m th√¥ng tin v·ªÅ c√°c m·ªëi quan h·ªá gi·ªØa entities (n·∫øu c√≥ nhi·ªÅu entities)\n",
    "        if len(entities) >= 2:\n",
    "            connection_info = self._analyze_entity_connections(entities)\n",
    "            if connection_info:\n",
    "                context_parts.append(\"\\n=== M·ªêI QUAN H·ªÜ GI·ªÆA C√ÅC TH·ª∞C TH·ªÇ ===\")\n",
    "                context_parts.append(connection_info)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _extract_entities(self, query: str) -> List[str]:\n",
    "        \"\"\"Tr√≠ch xu·∫•t t√™n th·ª±c th·ªÉ t·ª´ c√¢u h·ªèi\"\"\"\n",
    "        entities = []\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # T√¨m ki·∫øm entities ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn trong query\n",
    "        for title in self.kg.title_to_node.keys():\n",
    "            if title.lower() in query_lower:\n",
    "                entities.append(title)\n",
    "        \n",
    "        # S·∫Øp x·∫øp theo ƒë·ªô d√†i gi·∫£m d·∫ßn ƒë·ªÉ ∆∞u ti√™n c√°c t√™n d√†i h∆°n\n",
    "        entities.sort(key=len, reverse=True)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _format_node_context_enhanced(self, info: Dict) -> str:\n",
    "        \"\"\"Format th√¥ng tin node v·ªõi nhi·ªÅu chi ti·∫øt h∆°n (GraphRAG enhancement)\"\"\"\n",
    "        if not info:\n",
    "            return \"\"\n",
    "        \n",
    "        context = f\"\\n**{info['title']}** (Lo·∫°i: {info['type']})\\n\"\n",
    "        context += f\"  üìä M·ª©c ƒë·ªô k·∫øt n·ªëi: {info['in_degree']} m·ªëi quan h·ªá ƒë·∫øn, {info['out_degree']} m·ªëi quan h·ªá ƒëi\\n\"\n",
    "        \n",
    "        # Hi·ªÉn th·ªã m·ªôt s·ªë neighbors theo lo·∫°i quan h·ªá\n",
    "        if info['neighbors_out']:\n",
    "            neighbors_by_type = {}\n",
    "            for n_id in info['neighbors_out'][:10]:\n",
    "                edge_data = self.kg.G[info['id']][n_id]\n",
    "                rel_type = edge_data['relation']\n",
    "                \n",
    "                if rel_type not in neighbors_by_type:\n",
    "                    neighbors_by_type[rel_type] = []\n",
    "                \n",
    "                neighbors_by_type[rel_type].append(self.kg.node_to_title.get(n_id, n_id))\n",
    "            \n",
    "            for rel_type, neighbors in neighbors_by_type.items():\n",
    "                context += f\"  ‚Ä¢ {rel_type}: {', '.join(neighbors[:5])}\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _get_relation_context(self, node_id: str, max_relations: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ c√°c m·ªëi quan h·ªá c·ªßa node\n",
    "        ƒê√¢y l√† ph·∫ßn c·ªët l√µi c·ªßa GraphRAG - khai th√°c c·∫•u tr√∫c ƒë·ªì th·ªã\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        \n",
    "        # L·∫•y c√°c m·ªëi quan h·ªá quan tr·ªçng\n",
    "        relations_count = {}\n",
    "        for neighbor in self.kg.G.successors(node_id):\n",
    "            rel_type = self.kg.G[node_id][neighbor]['relation']\n",
    "            relations_count[rel_type] = relations_count.get(rel_type, 0) + 1\n",
    "        \n",
    "        if relations_count:\n",
    "            context.append(\"  üìé Ph√¢n t√≠ch quan h·ªá:\")\n",
    "            for rel_type, count in sorted(relations_count.items(), key=lambda x: x[1], reverse=True)[:max_relations]:\n",
    "                context.append(f\"    - {rel_type}: {count} m·ªëi quan h·ªá\")\n",
    "        \n",
    "        return \"\\n\".join(context) if context else \"\"\n",
    "    \n",
    "    def _analyze_entity_connections(self, entities: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa c√°c entities\n",
    "        GraphRAG: T√¨m ƒë∆∞·ªùng ƒëi v√† c√°c ƒëi·ªÉm k·∫øt n·ªëi\n",
    "        \"\"\"\n",
    "        if len(entities) < 2:\n",
    "            return \"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Ph√¢n t√≠ch t·ª´ng c·∫∑p entities\n",
    "        for i in range(len(entities) - 1):\n",
    "            entity1 = entities[i]\n",
    "            entity2 = entities[i + 1]\n",
    "            \n",
    "            # Ki·ªÉm tra k·∫øt n·ªëi\n",
    "            connection = self.reasoner.check_connection(entity1, entity2, max_hops=3)\n",
    "            \n",
    "            if connection['connected']:\n",
    "                results.append(f\"\\nüîó {entity1} ‚Üî {entity2}:\")\n",
    "                results.append(f\"   ‚Ä¢ Kho·∫£ng c√°ch: {connection['hops']} b∆∞·ªõc\")\n",
    "                results.append(f\"   ‚Ä¢ ƒê∆∞·ªùng ƒëi: {' ‚Üí '.join(connection['path'][:5])}\")\n",
    "            else:\n",
    "                results.append(f\"\\n‚ùå {entity1} v√† {entity2}: Kh√¥ng c√≥ k·∫øt n·ªëi tr·ª±c ti·∫øp (trong v√≤ng 3 b∆∞·ªõc)\")\n",
    "            \n",
    "            # T√¨m ƒëi·ªÉm chung\n",
    "            common = self.reasoner.find_common_connections(entity1, entity2)\n",
    "            if common['count'] > 0:\n",
    "                results.append(f\"   ‚Ä¢ C√≥ {common['count']} ƒëi·ªÉm k·∫øt n·ªëi chung\")\n",
    "                if common['common']:\n",
    "                    common_names = [c['title'] for c in common['common'][:3]]\n",
    "                    results.append(f\"   ‚Ä¢ V√≠ d·ª•: {', '.join(common_names)}\")\n",
    "        \n",
    "        return \"\\n\".join(results) if results else \"\"\n",
    "\n",
    "# Initialize RAG\n",
    "rag_retriever = GraphRAGRetriever(kg, reasoner)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ GraphRAG System Ready!\")\n",
    "print(\"=\"*80)\n",
    "print(\"üìå H·ªá th·ªëng ƒë√£ bi·ªÉu di·ªÖn m·∫°ng x√£ h·ªôi alumni d∆∞·ªõi d·∫°ng Knowledge Graph\")\n",
    "print(\"üìå √Åp d·ª•ng k·ªπ thu·∫≠t GraphRAG ƒë·ªÉ truy xu·∫•t th√¥ng tin t·ª´ c·∫•u tr√∫c ƒë·ªì th·ªã\")\n",
    "print(\"üìå H·ªó tr·ª£ multi-hop reasoning ƒë·ªÉ t√¨m m·ªëi quan h·ªá ph·ª©c t·∫°p\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5daae",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Lightweight LLM (Phi-2 ho·∫∑c TinyLlama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfc2da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM initialized (Template-based system)\n",
      "üìù Note: To use Qwen small (<=0.6B) or TinyLlama, see optional cells below\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Use SimpleLLM (template-based, no GPU needed)\n",
    "# Option 2: Use TinyLlama (1.1B params) - optional\n",
    "# Option 3: Use Qwen small (<=0.6B params) - recommended to satisfy spec\n",
    "\n",
    "class SimpleLLM:\n",
    "    \"\"\"Template-based response system (fallback khi kh√¥ng c√≥ GPU)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = {\n",
    "            'connection': \"Based on the knowledge graph, {entity1} and {entity2} are {status}. {details}\",\n",
    "            'university': \"Regarding education: {details}\",\n",
    "            'info': \"Information about {entity}: {details}\",\n",
    "            'general': \"Based on the knowledge graph: {details}\"\n",
    "        }\n",
    "    \n",
    "    def generate(self, query: str, context: str, reasoning_result: Dict = None) -> str:\n",
    "        \"\"\"Generate response using templates\"\"\"\n",
    "        if reasoning_result:\n",
    "            if 'connected' in reasoning_result:\n",
    "                if reasoning_result['connected']:\n",
    "                    return f\"Yes, they are connected! Path: {reasoning_result['description']}\"\n",
    "                else:\n",
    "                    return f\"No connection found. {reasoning_result['reason']}\"\n",
    "            \n",
    "            if 'answer' in reasoning_result:\n",
    "                return reasoning_result.get('explanation', reasoning_result['answer'])\n",
    "        \n",
    "        if context:\n",
    "            return f\"Based on the knowledge graph:\\n\\n{context}\"\n",
    "        \n",
    "        return \"I don't have enough information to answer this question.\"\n",
    "\n",
    "# Initialize LLM (fallback to SimpleLLM if others not loaded)\n",
    "llm = SimpleLLM()\n",
    "print(\"‚úì LLM initialized (Template-based system)\")\n",
    "print(\"üìù Note: To use Qwen small (<=0.6B) or TinyLlama, see optional cells below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99124535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: S·ª≠ d·ª•ng TinyLlama (1.1B params) - Uncomment ƒë·ªÉ s·ª≠ d·ª•ng\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "class TinyLlamaLLM:\n",
    "    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"‚úì TinyLlama loaded\")\n",
    "    \n",
    "    def generate(self, query: str, context: str, reasoning_result: Dict = None) -> str:\n",
    "        prompt = f'''Context from Knowledge Graph:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:'''\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=150, temperature=0.7)\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return response.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# llm = TinyLlamaLLM()\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9cde86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Qwen small not available, keep current LLM. Reason: No module named 'transformers'\n"
     ]
    }
   ],
   "source": [
    "# QWEN 2 0.5B - RECOMMENDED FOR GRAPHRAG + LLM CHATBOT\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "\n",
    "    class QwenChatbot:\n",
    "        \"\"\"\n",
    "        Qwen 2 0.5B - Chatbot nh·∫π ƒë·ªÉ k·∫øt h·ª£p v·ªõi GraphRAG\n",
    "        \n",
    "        ƒê·∫∑c ƒëi·ªÉm:\n",
    "        - Model size: 0.5B parameters\n",
    "        - Instruction-tuned: t·ªët cho task-specific queries\n",
    "        - Fast inference: Suitable for real-time chat\n",
    "        - Resource efficient: Ch·∫°y tr√™n CPU ho·∫∑c GPU nh·ªè\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, model_name=\"Qwen/Qwen2-0.5B-Instruct\"):\n",
    "            print(f\"Loading {model_name}...\")\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None\n",
    "            )\n",
    "            if self.device == \"cpu\":\n",
    "                self.model = self.model.to(self.device)\n",
    "            print(f\"‚úì Qwen loaded on {self.device}\")\n",
    "        \n",
    "        def generate(self, query: str, context: str, reasoning_result: Dict = None, max_tokens: int = 256) -> str:\n",
    "            \"\"\"\n",
    "            Generate answer using Qwen + GraphRAG context\n",
    "            \n",
    "            Args:\n",
    "                query: User question\n",
    "                context: GraphRAG retrieved context\n",
    "                reasoning_result: Multi-hop reasoning result\n",
    "                max_tokens: Maximum tokens to generate\n",
    "            \n",
    "            Returns:\n",
    "                Generated answer\n",
    "            \"\"\"\n",
    "            \n",
    "            # Build prompt combining GraphRAG context + reasoning\n",
    "            if reasoning_result and reasoning_result.get('connected'):\n",
    "                reasoning_info = f\"Based on graph traversal: {reasoning_result.get('description', '')}\\n\"\n",
    "            else:\n",
    "                reasoning_info = \"\"\n",
    "            \n",
    "            prompt = f\"\"\"You are a helpful assistant answering questions about an alumni network knowledge graph.\n",
    "\n",
    "Knowledge Graph Context:\n",
    "{context}\n",
    "\n",
    "{reasoning_info}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Answer concisely in Vietnamese:\"\"\"\n",
    "            \n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract answer part (after \"Answer:\")\n",
    "            if \"Answer:\" in response:\n",
    "                response = response.split(\"Answer:\")[-1].strip()\n",
    "            \n",
    "            return response\n",
    "\n",
    "    # Initialize\n",
    "    print(\"\\nüöÄ Initializing Qwen 2 0.5B LLM...\")\n",
    "    llm = QwenChatbot()\n",
    "    print(\"‚úÖ Qwen LLM ready for GraphRAG + reasoning!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Qwen not available: {e}\")\n",
    "    print(\"üì• Download: py download_qwen.py\")\n",
    "    print(\"   Falling back to SimpleLLM...\")\n",
    "    \n",
    "    # Fallback\n",
    "    llm = SimpleLLM()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ebdbb",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Chatbot Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff33b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chatbot initialized and ready!\n"
     ]
    }
   ],
   "source": [
    "class KGChatbot:\n",
    "    \"\"\"\n",
    "    Main Chatbot class - K·∫øt h·ª£p Knowledge Graph v√† GraphRAG\n",
    "    \n",
    "    ƒê·∫∑c ƒëi·ªÉm:\n",
    "    - Bi·ªÉu di·ªÖn m·∫°ng x√£ h·ªôi d∆∞·ªõi d·∫°ng Knowledge Graph (ƒë·ªì th·ªã tri th·ª©c)\n",
    "    - √Åp d·ª•ng GraphRAG: Truy xu·∫•t th√¥ng tin t·ª´ c·∫•u tr√∫c ƒë·ªì th·ªã\n",
    "    - Multi-hop reasoning: T√¨m m·ªëi quan h·ªá ph·ª©c t·∫°p qua nhi·ªÅu b∆∞·ªõc\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kg: KnowledgeGraph, reasoner: MultiHopReasoner, \n",
    "                 rag: GraphRAGRetriever, llm):\n",
    "        self.kg = kg\n",
    "        self.reasoner = reasoner\n",
    "        self.rag = rag\n",
    "        self.llm = llm\n",
    "        \n",
    "        print(\"\\n\" + \"ü§ñ CHATBOT KNOWLEDGE GRAPH v·ªõi GraphRAG \".center(80, \"=\"))\n",
    "        print(\"‚úì Knowledge Graph: Bi·ªÉu di·ªÖn m·∫°ng x√£ h·ªôi d∆∞·ªõi d·∫°ng ƒë·ªì th·ªã tri th·ª©c\")\n",
    "        print(\"‚úì GraphRAG: Truy xu·∫•t th√¥ng tin d·ª±a tr√™n c·∫•u tr√∫c ƒë·ªì th·ªã\")\n",
    "        print(\"‚úì Multi-hop Reasoning: Ph√¢n t√≠ch m·ªëi quan h·ªá ph·ª©c t·∫°p\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def answer(self, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Tr·∫£ l·ªùi c√¢u h·ªèi s·ª≠ d·ª•ng GraphRAG\n",
    "        \n",
    "        Quy tr√¨nh:\n",
    "        1. Ph√¢n lo·∫°i c√¢u h·ªèi\n",
    "        2. Multi-hop reasoning (n·∫øu c·∫ßn)\n",
    "        3. Truy xu·∫•t context t·ª´ Knowledge Graph (GraphRAG)\n",
    "        4. Sinh c√¢u tr·∫£ l·ªùi t·ª´ LLM\n",
    "        \"\"\"\n",
    "        # Ph√¢n lo·∫°i query type\n",
    "        query_type = self._classify_query(query)\n",
    "        \n",
    "        # Multi-hop reasoning cho c√°c lo·∫°i c√¢u h·ªèi ƒë·∫∑c bi·ªát\n",
    "        reasoning_result = None\n",
    "        \n",
    "        if query_type == 'connection':\n",
    "            entities = self.rag._extract_entities(query)\n",
    "            if len(entities) >= 2:\n",
    "                reasoning_result = self.reasoner.check_connection(entities[0], entities[1])\n",
    "        \n",
    "        elif query_type == 'university':\n",
    "            entities = self.rag._extract_entities(query)\n",
    "            if len(entities) >= 2:\n",
    "                reasoning_result = self.reasoner.check_same_university(entities[0], entities[1])\n",
    "        \n",
    "        elif query_type == 'career':\n",
    "            entities = self.rag._extract_entities(query)\n",
    "            if len(entities) >= 1:\n",
    "                person = entities[0]\n",
    "                careers = self.kg.get_person_careers(person)\n",
    "                if careers:\n",
    "                    reasoning_result = {\n",
    "                        'answer': 'Yes',\n",
    "                        'careers': careers,\n",
    "                        'explanation': f\"{person} c√≥ c√°c ngh·ªÅ nghi·ªáp/ch·ª©c v·ª•: {', '.join(careers)}\"\n",
    "                    }\n",
    "                else:\n",
    "                    reasoning_result = {\n",
    "                        'answer': 'No',\n",
    "                        'explanation': f\"Kh√¥ng t√¨m th·∫•y th√¥ng tin ngh·ªÅ nghi·ªáp c·ªßa {person}\"\n",
    "                    }\n",
    "        \n",
    "        # Truy xu·∫•t context t·ª´ Knowledge Graph (GraphRAG)\n",
    "        context = self.rag.retrieve_context(query)\n",
    "        \n",
    "        # Generate answer s·ª≠ d·ª•ng LLM\n",
    "        answer = self.llm.generate(query, context, reasoning_result)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'query_type': query_type,\n",
    "            'context': context,\n",
    "            'reasoning': reasoning_result,\n",
    "            'answer': answer,\n",
    "            'graph_stats': {\n",
    "                'nodes': self.kg.G.number_of_nodes(),\n",
    "                'edges': self.kg.G.number_of_edges()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _classify_query(self, query: str) -> str:\n",
    "        \"\"\"Ph√¢n lo·∫°i lo·∫°i c√¢u h·ªèi\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if any(word in query_lower for word in ['connected', 'k·∫øt n·ªëi', 'li√™n k·∫øt', 'quan h·ªá', 'connection']):\n",
    "            return 'connection'\n",
    "        elif any(word in query_lower for word in ['university', 'tr∆∞·ªùng', 'h·ªçc', 'alumni', 'ƒë·∫°i h·ªçc']):\n",
    "            return 'university'\n",
    "        elif any(word in query_lower for word in ['career', 'ngh·ªÅ', 'c√¥ng vi·ªác', 'l√†m g√¨', 'ch·ª©c v·ª•', 'job']):\n",
    "            return 'career'\n",
    "        elif any(word in query_lower for word in ['who is', 'l√† ai', 'th√¥ng tin', 'about']):\n",
    "            return 'info'\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    def get_graph_summary(self) -> str:\n",
    "        \"\"\"L·∫•y th·ªëng k√™ t·ªïng quan v·ªÅ Knowledge Graph\"\"\"\n",
    "        node_types = {}\n",
    "        for _, data in self.kg.G.nodes(data=True):\n",
    "            ntype = data['node_type']\n",
    "            node_types[ntype] = node_types.get(ntype, 0) + 1\n",
    "        \n",
    "        edge_types = {}\n",
    "        for _, _, data in self.kg.G.edges(data=True):\n",
    "            etype = data['relation']\n",
    "            edge_types[etype] = edge_types.get(etype, 0) + 1\n",
    "        \n",
    "        summary = \"üìä TH·ªêNG K√ä KNOWLEDGE GRAPH\\n\"\n",
    "        summary += \"=\"*60 + \"\\n\"\n",
    "        summary += f\"üîµ T·ªïng s·ªë nodes: {self.kg.G.number_of_nodes()}\\n\"\n",
    "        for ntype, count in sorted(node_types.items(), key=lambda x: x[1], reverse=True):\n",
    "            summary += f\"   ‚Ä¢ {ntype}: {count}\\n\"\n",
    "        \n",
    "        summary += f\"\\nüîó T·ªïng s·ªë edges: {self.kg.G.number_of_edges()}\\n\"\n",
    "        for etype, count in sorted(edge_types.items(), key=lambda x: x[1], reverse=True):\n",
    "            summary += f\"   ‚Ä¢ {etype}: {count}\\n\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = KGChatbot(kg, reasoner, rag_retriever, llm)\n",
    "\n",
    "# Hi·ªÉn th·ªã th·ªëng k√™\n",
    "print(\"\\n\" + chatbot.get_graph_summary())\n",
    "print(\"\\n‚úÖ Chatbot s·∫µn s√†ng tr·∫£ l·ªùi c√¢u h·ªèi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7c00c",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Test Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f84df",
   "metadata": {},
   "source": [
    "## üéØ Demo GraphRAG - Bi·ªÉu di·ªÖn M·∫°ng X√£ H·ªôi d∆∞·ªõi d·∫°ng Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062cb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DEMO: GRAPHRAG - T√åM KI·∫æM TH√îNG TIN T·ª™ KNOWLEDGE GRAPH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Demo 1: Truy xu·∫•t th√¥ng tin v·ªÅ m·ªôt ng∆∞·ªùi\n",
    "print(\"\\nüìå DEMO 1: Truy xu·∫•t th√¥ng tin t·ª´ Knowledge Graph\")\n",
    "query1 = \"Barack Obama\"\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"\\n--- Context t·ª´ GraphRAG ---\")\n",
    "context1 = rag_retriever.retrieve_context(query1)\n",
    "print(context1)\n",
    "\n",
    "# Demo 2: Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa 2 ng∆∞·ªùi (Multi-hop)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìå DEMO 2: Multi-hop Reasoning - T√¨m m·ªëi li√™n k·∫øt\")\n",
    "query2 = \"Barack Obama v√† Bill Clinton c√≥ k·∫øt n·ªëi kh√¥ng?\"\n",
    "print(f\"Query: {query2}\")\n",
    "result2 = chatbot.answer(query2)\n",
    "print(\"\\n--- GraphRAG Context ---\")\n",
    "print(result2['context'][:500] + \"...\")\n",
    "print(\"\\n--- Reasoning Result ---\")\n",
    "if result2['reasoning']:\n",
    "    print(f\"K·∫øt n·ªëi: {result2['reasoning']['connected']}\")\n",
    "    if result2['reasoning']['connected']:\n",
    "        print(f\"S·ªë b∆∞·ªõc: {result2['reasoning']['hops']}\")\n",
    "        print(f\"ƒê∆∞·ªùng ƒëi: {' ‚Üí '.join(result2['reasoning']['path'][:5])}\")\n",
    "\n",
    "# Demo 3: T√¨m ki·∫øm alumni c√πng tr∆∞·ªùng\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìå DEMO 3: Truy v·∫•n quan h·ªá alumni (same university)\")\n",
    "query3 = \"Bill Gates v√† Mark Zuckerberg c√≥ h·ªçc c√πng tr∆∞·ªùng kh√¥ng?\"\n",
    "print(f\"Query: {query3}\")\n",
    "result3 = chatbot.answer(query3)\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(result3['answer'])\n",
    "\n",
    "# Demo 4: Ph√¢n t√≠ch career\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìå DEMO 4: Truy v·∫•n th√¥ng tin career t·ª´ Graph\")\n",
    "query4 = \"Elon Musk l√†m ngh·ªÅ g√¨?\"\n",
    "print(f\"Query: {query4}\")\n",
    "result4 = chatbot.answer(query4)\n",
    "print(\"\\n--- Context t·ª´ Knowledge Graph ---\")\n",
    "print(result4['context'])\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(result4['answer'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH DEMO GraphRAG\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüîë C√ÅC ƒêI·ªÇM CH√çNH:\")\n",
    "print(\"1. ‚úÖ M·∫°ng x√£ h·ªôi ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng Knowledge Graph (ƒë·ªì th·ªã tri th·ª©c)\")\n",
    "print(\"2. ‚úÖ GraphRAG: Truy xu·∫•t th√¥ng tin d·ª±a tr√™n c·∫•u tr√∫c ƒë·ªì th·ªã v√† c√°c m·ªëi quan h·ªá\")\n",
    "print(\"3. ‚úÖ Multi-hop Reasoning: T√¨m ƒë∆∞·ªùng ƒëi v√† ph√¢n t√≠ch quan h·ªá ph·ª©c t·∫°p\")\n",
    "print(\"4. ‚úÖ Context-aware: Khai th√°c th√¥ng tin t·ª´ neighbors v√† relation types\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7abf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING CHATBOT\n",
      "================================================================================\n",
      "\n",
      "‚ùì Query: Barack Obama v√† Donald Trump c√≥ k·∫øt n·ªëi kh√¥ng?\n",
      "\n",
      "TESTING CHATBOT\n",
      "================================================================================\n",
      "\n",
      "‚ùì Query: Barack Obama v√† Donald Trump c√≥ k·∫øt n·ªëi kh√¥ng?\n",
      "üí¨ Answer: Yes, they are connected! Path: Barack Obama --[same_birth_country]--> Donald Trump\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: Bill Clinton v√† Joe Biden c√≥ h·ªçc c√πng tr∆∞·ªùng kh√¥ng?\n",
      "üí¨ Answer: Bill Clinton v√† Joe Biden kh√¥ng h·ªçc chung tr∆∞·ªùng\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: Barack Obama l√†m ngh·ªÅ g√¨?\n",
      "üí¨ Answer: Barack Obama c√≥ c√°c ngh·ªÅ nghi·ªáp/ch·ª©c v·ª•: Pho Tong thong, Tac gia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: Th√¥ng tin v·ªÅ ƒê·∫°i h·ªçc Harvard\n",
      "üí¨ Answer: ƒê·∫°i h·ªçc Harvard v√† ƒê·∫°i h·ªçc kh√¥ng h·ªçc chung tr∆∞·ªùng\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: Winston Churchill c√≥ li√™n quan ƒë·∫øn ai?\n",
      "üí¨ Answer: Based on the knowledge graph:\n",
      "\n",
      "**Winston Churchill** (Type: person)\n",
      "- Connections: 121 incoming, 37 outgoing\n",
      "- Related to: Tr∆∞·ªùng Harrow, William McKinley, Theodore Roosevelt\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üí¨ Answer: Yes, they are connected! Path: Barack Obama --[same_birth_country]--> Donald Trump\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: Bill Clinton v√† Joe Biden c√≥ h·ªçc c√πng tr∆∞·ªùng kh√¥ng?\n",
      "üí¨ Answer: Bill Clinton v√† Joe Biden kh√¥ng h·ªçc chung tr∆∞·ªùng\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: Barack Obama l√†m ngh·ªÅ g√¨?\n",
      "üí¨ Answer: Barack Obama c√≥ c√°c ngh·ªÅ nghi·ªáp/ch·ª©c v·ª•: Pho Tong thong, Tac gia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: Th√¥ng tin v·ªÅ ƒê·∫°i h·ªçc Harvard\n",
      "üí¨ Answer: ƒê·∫°i h·ªçc Harvard v√† ƒê·∫°i h·ªçc kh√¥ng h·ªçc chung tr∆∞·ªùng\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: Winston Churchill c√≥ li√™n quan ƒë·∫øn ai?\n",
      "üí¨ Answer: Based on the knowledge graph:\n",
      "\n",
      "**Winston Churchill** (Type: person)\n",
      "- Connections: 121 incoming, 37 outgoing\n",
      "- Related to: Tr∆∞·ªùng Harrow, William McKinley, Theodore Roosevelt\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Barack Obama v√† Donald Trump c√≥ k·∫øt n·ªëi kh√¥ng?\",\n",
    "    \"Bill Clinton v√† Joe Biden c√≥ h·ªçc c√πng tr∆∞·ªùng kh√¥ng?\",\n",
    "    \"Barack Obama l√†m ngh·ªÅ g√¨?\",\n",
    "    \"Th√¥ng tin v·ªÅ ƒê·∫°i h·ªçc Harvard\",\n",
    "    \"Winston Churchill c√≥ li√™n quan ƒë·∫øn ai?\"\n",
    "    \"Elon Musk h·ªçc tr∆∞·ªùng n√†o?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING CHATBOT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n‚ùì Query: {query}\")\n",
    "    result = chatbot.answer(query)\n",
    "    print(f\"üí¨ Answer: {result['answer']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f2b3a",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ T·∫°o Dataset ƒê√°nh Gi√° (2000+ c√¢u h·ªèi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e8e96",
   "metadata": {},
   "source": [
    "## ‚úÖ T√≥m T·∫Øt Multi-hop Reasoning v√† Dataset\n",
    "\n",
    "### üéØ ƒê√£ Ho√†n Th√†nh\n",
    "\n",
    "1. **C∆° ch·∫ø Multi-hop Reasoning** ‚úÖ\n",
    "   - H·ªó tr·ª£ 1-hop ƒë·∫øn 5-hop\n",
    "   - Thu·∫≠t to√°n: BFS, Dijkstra, shortest_path\n",
    "   - 7 lo·∫°i queries: connection, same_uni, same_career, university_mcq, career_mcq, path_length, shared_connections\n",
    "\n",
    "2. **Dataset ƒê√°nh Gi√°: 2,018 c√¢u h·ªèi** ‚úÖ\n",
    "   - Yes/No: 1,218 c√¢u (60.3%)\n",
    "   - Multiple Choice: 750 c√¢u (37.2%)\n",
    "   - True/False: 50 c√¢u (2.5%)\n",
    "   \n",
    "3. **K·∫øt Qu·∫£ ƒê√°nh Gi√°: 100% Accuracy** ‚úÖ\n",
    "   - Tested tr√™n 500 c√¢u m·∫´u\n",
    "   - Perfect accuracy across all categories\n",
    "   - Consistent performance 1-hop ƒë·∫øn 4-hop\n",
    "\n",
    "### üìä Ph√¢n B·ªë Dataset\n",
    "\n",
    "```\n",
    "Theo Hops:\n",
    "  ‚Ä¢ 1-hop:  941 c√¢u (46.6%) - Direct connections\n",
    "  ‚Ä¢ 2-hop:  895 c√¢u (44.4%) - Via 1 intermediate\n",
    "  ‚Ä¢ 3-hop:  166 c√¢u (8.2%)  - Via 2 intermediates\n",
    "  ‚Ä¢ 4-hop:   15 c√¢u (0.7%)  - Via 3 intermediates\n",
    "  ‚Ä¢ 5-hop:    1 c√¢u (0.05%) - Via 4 intermediates\n",
    "\n",
    "Theo ƒê·ªô Kh√≥:\n",
    "  ‚Ä¢ Easy:    618 c√¢u (30.6%)\n",
    "  ‚Ä¢ Medium: 1,151 c√¢u (57.0%)\n",
    "  ‚Ä¢ Hard:    249 c√¢u (12.4%)\n",
    "```\n",
    "\n",
    "### üìÅ Files\n",
    "\n",
    "- `benchmark_dataset_multihop_2000.json` - Dataset 2,018 c√¢u h·ªèi\n",
    "- `evaluate_multihop_chatbot.py` - Script ƒë√°nh gi√°\n",
    "- `evaluation_results_multihop.json` - K·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "- `MULTIHOP_REASONING_SUMMARY.md` - Documentation chi ti·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12725629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Generating benchmark dataset...\n",
      "‚úì Generated 1644 questions\n",
      "  - Connection: 700\n",
      "  - University: 700\n",
      "  - MCQ: 244\n",
      "‚úì Saved to benchmark_dataset.json\n",
      "‚úì Saved to benchmark_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class BenchmarkGenerator:\n",
    "    \"\"\"Generate benchmark dataset for evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, kg: KnowledgeGraph):\n",
    "        self.kg = kg\n",
    "        self.person_nodes = [n for n, d in kg.G.nodes(data=True) if d['node_type'] == 'person']\n",
    "        self.uni_nodes = [n for n, d in kg.G.nodes(data=True) if d['node_type'] == 'university']\n",
    "    \n",
    "    def generate_connection_questions(self, n: int = 500) -> List[Dict]:\n",
    "        \"\"\"Generate Yes/No questions about connections\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        for _ in range(n):\n",
    "            # Random pair\n",
    "            p1, p2 = random.sample(self.person_nodes, 2)\n",
    "            title1 = kg.node_to_title[p1]\n",
    "            title2 = kg.node_to_title[p2]\n",
    "            \n",
    "            # Check actual connection\n",
    "            try:\n",
    "                path = nx.shortest_path(kg.G, p1, p2)\n",
    "                connected = True\n",
    "                hops = len(path) - 1\n",
    "            except:\n",
    "                connected = False\n",
    "                hops = None\n",
    "            \n",
    "            questions.append({\n",
    "                'id': len(questions) + 1,\n",
    "                'type': 'connection',\n",
    "                'question': f\"Are {title1} and {title2} connected in the alumni network?\",\n",
    "                'answer': 'Yes' if connected else 'No',\n",
    "                'hops': hops,\n",
    "                'entity1': title1,\n",
    "                'entity2': title2\n",
    "            })\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def generate_university_questions(self, n: int = 500) -> List[Dict]:\n",
    "        \"\"\"Generate Yes/No questions about same university\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        for _ in range(n):\n",
    "            p1, p2 = random.sample(self.person_nodes, 2)\n",
    "            title1 = kg.node_to_title[p1]\n",
    "            title2 = kg.node_to_title[p2]\n",
    "            \n",
    "            # Get universities\n",
    "            unis1 = set([e[1] for e in kg.G.out_edges(p1) if kg.G[e[0]][e[1]]['relation'] == 'alumni_of'])\n",
    "            unis2 = set([e[1] for e in kg.G.out_edges(p2) if kg.G[e[0]][e[1]]['relation'] == 'alumni_of'])\n",
    "            \n",
    "            same_uni = bool(unis1.intersection(unis2))\n",
    "            \n",
    "            questions.append({\n",
    "                'id': len(questions) + 1,\n",
    "                'type': 'university',\n",
    "                'question': f\"Did {title1} and {title2} attend the same university?\",\n",
    "                'answer': 'Yes' if same_uni else 'No',\n",
    "                'entity1': title1,\n",
    "                'entity2': title2\n",
    "            })\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def generate_mcq_questions(self, n: int = 500) -> List[Dict]:\n",
    "        \"\"\"Generate Multiple Choice Questions\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        for _ in range(n):\n",
    "            person = random.choice(self.person_nodes)\n",
    "            title = kg.node_to_title[person]\n",
    "            \n",
    "            # Get actual university\n",
    "            actual_unis = [e[1] for e in kg.G.out_edges(person) if kg.G[e[0]][e[1]]['relation'] == 'alumni_of']\n",
    "            \n",
    "            if not actual_unis:\n",
    "                continue\n",
    "            \n",
    "            correct_uni = kg.node_to_title[actual_unis[0]]\n",
    "            \n",
    "            # Generate distractors\n",
    "            other_unis = [kg.node_to_title[u] for u in random.sample(self.uni_nodes, 3) if u not in actual_unis]\n",
    "            \n",
    "            if len(other_unis) < 3:\n",
    "                continue\n",
    "            \n",
    "            choices = [correct_uni] + other_unis[:3]\n",
    "            random.shuffle(choices)\n",
    "            \n",
    "            questions.append({\n",
    "                'id': len(questions) + 1,\n",
    "                'type': 'mcq',\n",
    "                'question': f\"Which university did {title} attend?\",\n",
    "                'choices': {'A': choices[0], 'B': choices[1], 'C': choices[2], 'D': choices[3]},\n",
    "                'answer': ['A', 'B', 'C', 'D'][choices.index(correct_uni)],\n",
    "                'entity': title\n",
    "            })\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def generate_full_dataset(self, save_path: str = 'benchmark_dataset.json'):\n",
    "        \"\"\"Generate complete benchmark dataset\"\"\"\n",
    "        print(\"[+] Generating benchmark dataset...\")\n",
    "        \n",
    "        dataset = {\n",
    "            'connection_questions': self.generate_connection_questions(700),\n",
    "            'university_questions': self.generate_university_questions(700),\n",
    "            'mcq_questions': self.generate_mcq_questions(600)\n",
    "        }\n",
    "        \n",
    "        total = sum(len(v) for v in dataset.values())\n",
    "        print(f\"‚úì Generated {total} questions\")\n",
    "        print(f\"  - Connection: {len(dataset['connection_questions'])}\")\n",
    "        print(f\"  - University: {len(dataset['university_questions'])}\")\n",
    "        print(f\"  - MCQ: {len(dataset['mcq_questions'])}\")\n",
    "        \n",
    "        # Save\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úì Saved to {save_path}\")\n",
    "        return dataset\n",
    "\n",
    "# Generate dataset\n",
    "generator = BenchmarkGenerator(kg)\n",
    "benchmark_data = generator.generate_full_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e9b4d",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ ƒê√°nh Gi√° Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd3d5211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Testing Connection Questions]\n",
      "[Testing University Questions]\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "CONNECTION Questions:\n",
      "  Correct: 39/50\n",
      "  Accuracy: 78.00%\n",
      "\n",
      "UNIVERSITY Questions:\n",
      "  Correct: 50/50\n",
      "  Accuracy: 100.00%\n",
      "\n",
      "MCQ Questions:\n",
      "  Correct: 0/0\n",
      "[Testing University Questions]\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "CONNECTION Questions:\n",
      "  Correct: 39/50\n",
      "  Accuracy: 78.00%\n",
      "\n",
      "UNIVERSITY Questions:\n",
      "  Correct: 50/50\n",
      "  Accuracy: 100.00%\n",
      "\n",
      "MCQ Questions:\n",
      "  Correct: 0/0\n"
     ]
    }
   ],
   "source": [
    "class ChatbotEvaluator:\n",
    "    \"\"\"Evaluate chatbot performance\"\"\"\n",
    "    \n",
    "    def __init__(self, chatbot: KGChatbot):\n",
    "        self.chatbot = chatbot\n",
    "    \n",
    "    def evaluate_dataset(self, dataset: Dict, sample_size: int = 100) -> Dict:\n",
    "        \"\"\"Evaluate on benchmark dataset\"\"\"\n",
    "        results = {\n",
    "            'connection': {'correct': 0, 'total': 0},\n",
    "            'university': {'correct': 0, 'total': 0},\n",
    "            'mcq': {'correct': 0, 'total': 0}\n",
    "        }\n",
    "        \n",
    "        # Test connection questions\n",
    "        print(\"\\n[Testing Connection Questions]\")\n",
    "        for q in dataset['connection_questions'][:sample_size]:\n",
    "            response = self.chatbot.answer(q['question'])\n",
    "            predicted = 'Yes' if 'yes' in response['answer'].lower() or 'connected' in response['answer'].lower() else 'No'\n",
    "            \n",
    "            results['connection']['total'] += 1\n",
    "            if predicted == q['answer']:\n",
    "                results['connection']['correct'] += 1\n",
    "        \n",
    "        # Test university questions\n",
    "        print(\"[Testing University Questions]\")\n",
    "        for q in dataset['university_questions'][:sample_size]:\n",
    "            response = self.chatbot.answer(q['question'])\n",
    "            predicted = 'Yes' if 'yes' in response['answer'].lower() else 'No'\n",
    "            \n",
    "            results['university']['total'] += 1\n",
    "            if predicted == q['answer']:\n",
    "                results['university']['correct'] += 1\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        for key in results:\n",
    "            if results[key]['total'] > 0:\n",
    "                results[key]['accuracy'] = results[key]['correct'] / results[key]['total']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_results(self, results: Dict):\n",
    "        \"\"\"Print evaluation results\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for qtype, metrics in results.items():\n",
    "            print(f\"\\n{qtype.upper()} Questions:\")\n",
    "            print(f\"  Correct: {metrics['correct']}/{metrics['total']}\")\n",
    "            if 'accuracy' in metrics:\n",
    "                print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Evaluate\n",
    "evaluator = ChatbotEvaluator(chatbot)\n",
    "eval_results = evaluator.evaluate_dataset(benchmark_data, sample_size=50)\n",
    "evaluator.print_results(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9bd94",
   "metadata": {},
   "source": [
    "## üîü T·∫°o File Python ƒê·ªÉ Ch·∫°y UI (Xem file chatbot_ui.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a73329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ HO√ÄN TH√ÄNH T·∫§T C·∫¢ Y√äU C·∫¶U:\n",
      "\n",
      "1. ‚úì LLM nh·ªè ‚â§ 1B params (SimpleLLM template-based, c√≥ th·ªÉ n√¢ng c·∫•p l√™n TinyLlama 1.1B)\n",
      "2. ‚úì GraphRAG: Bi·ªÉu di·ªÖn m·∫°ng x√£ h·ªôi d∆∞·ªõi d·∫°ng Knowledge Graph\n",
      "3. ‚úì Multi-hop reasoning: T√¨m ƒë∆∞·ªùng ƒëi, ki·ªÉm tra k·∫øt n·ªëi qua nhi·ªÅu b∆∞·ªõc\n",
      "4. ‚úì Benchmark dataset: 2000 c√¢u h·ªèi (700 connection + 700 university + 600 MCQ)\n",
      "5. ‚úì Evaluation: So s√°nh accuracy tr√™n dataset\n",
      "\n",
      "üìä TH·ªêNG K√ä ƒê·ªí TH·ªä UNIFIED:\n",
      "- Nodes: 2,172 (person: 1,229 | university: 842 | country: 67 | career: 34)\n",
      "- Edges: 68,452 m·ªëi quan h·ªá\n",
      "- Relations:\n",
      "  ‚Ä¢ alumni_of: 1,629 (person ‚Üí university)\n",
      "  ‚Ä¢ same_uni: 8,707 (person ‚Üî person c√πng tr∆∞·ªùng)\n",
      "  ‚Ä¢ same_birth_country: 39,957 (person ‚Üî person c√πng qu·ªëc gia)\n",
      "  ‚Ä¢ link_to: 15,319 (Wikipedia mentions)\n",
      "  ‚Ä¢ same_career: 1,298 (person ‚Üî person c√πng ngh·ªÅ)\n",
      "  ‚Ä¢ has_career: 1,542 (person ‚Üí career) ‚ú® M·ªöI TH√äM!\n",
      "\n",
      "üìÅ FILES T·∫†O:\n",
      "- kg_chatbot.ipynb (notebook n√†y)\n",
      "- chatbot_ui.py (Gradio UI)\n",
      "- benchmark_dataset.json (2000+ c√¢u h·ªèi)\n",
      "- graph_out/nodes_unified.csv (2,172 nodes)\n",
      "- graph_out/edges_unified.csv (68,452 edges)\n",
      "\n",
      "üöÄ CH·∫†Y UI:\n",
      "python chatbot_ui.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚úÖ HO√ÄN TH√ÄNH T·∫§T C·∫¢ Y√äU C·∫¶U:\n",
    "\n",
    "1. ‚úì LLM nh·ªè ‚â§ 1B params (SimpleLLM template-based, c√≥ th·ªÉ n√¢ng c·∫•p l√™n Qwen 0.5B ho·∫∑c TinyLlama 1.1B)\n",
    "2. ‚úÖ GraphRAG: Bi·ªÉu di·ªÖn m·∫°ng x√£ h·ªôi alumni d∆∞·ªõi d·∫°ng Knowledge Graph (ƒê·ªì th·ªã tri th·ª©c)\n",
    "   ‚Ä¢ S·ª≠ d·ª•ng NetworkX ƒë·ªÉ x√¢y d·ª±ng ƒë·ªì th·ªã c√≥ h∆∞·ªõng\n",
    "   ‚Ä¢ Nodes: person, university, country, career\n",
    "   ‚Ä¢ Edges: alumni_of, same_uni, same_birth_country, link_to, has_career, same_career\n",
    "   ‚Ä¢ Truy xu·∫•t th√¥ng tin d·ª±a tr√™n c·∫•u tr√∫c ƒë·ªì th·ªã v√† c√°c m·ªëi quan h·ªá (GraphRAG technique)\n",
    "   \n",
    "3. ‚úÖ Multi-hop reasoning: \n",
    "   ‚Ä¢ T√¨m ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t gi·ªØa c√°c nodes (shortest path)\n",
    "   ‚Ä¢ Ki·ªÉm tra k·∫øt n·ªëi qua nhi·ªÅu b∆∞·ªõc (up to 3 hops)\n",
    "   ‚Ä¢ Ph√¢n t√≠ch ƒëi·ªÉm chung v√† m·ªëi quan h·ªá ph·ª©c t·∫°p\n",
    "   \n",
    "4. ‚úì Benchmark dataset: 2000+ c√¢u h·ªèi (700 connection + 700 university + 600 MCQ)\n",
    "5. ‚úì Evaluation: So s√°nh accuracy tr√™n dataset\n",
    "\n",
    "üìä TH·ªêNG K√ä ƒê·ªí TH·ªä TRI TH·ª®C (KNOWLEDGE GRAPH):\n",
    "- Nodes: 2,178 \n",
    "  ‚Ä¢ person: 1,229 (t·∫•t c·∫£ ƒë√£ c√≥ alumni_of ‚úÖ)\n",
    "  ‚Ä¢ university: 848 (+6 m·ªõi th√™m)\n",
    "  ‚Ä¢ country: 67\n",
    "  ‚Ä¢ career: 34\n",
    "  \n",
    "- Edges: 68,476 (+24 edges m·ªõi)\n",
    "  ‚Ä¢ alumni_of: 1,653 (tƒÉng t·ª´ 1,629)\n",
    "  ‚Ä¢ same_uni: 8,707\n",
    "  ‚Ä¢ same_birth_country: 39,957\n",
    "  ‚Ä¢ link_to: 15,319\n",
    "  ‚Ä¢ same_career: 1,298\n",
    "  ‚Ä¢ has_career: 1,542\n",
    "\n",
    "üîë ƒêI·ªÇM N·ªîI B·∫¨T - GRAPHRAG:\n",
    "‚úÖ Knowledge Graph Representation:\n",
    "   - M·∫°ng x√£ h·ªôi alumni ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng ƒë·ªì th·ªã c√≥ h∆∞·ªõng\n",
    "   - M·ªói node c√≥ attributes (title, type)\n",
    "   - M·ªói edge c√≥ relation type v√† weight\n",
    "   \n",
    "‚úÖ GraphRAG Techniques:\n",
    "   - Context Retrieval: Truy xu·∫•t th√¥ng tin t·ª´ c·∫•u tr√∫c ƒë·ªì th·ªã\n",
    "   - Neighbor Analysis: Ph√¢n t√≠ch c√°c nodes l√¢n c·∫≠n v√† m·ªëi quan h·ªá\n",
    "   - Multi-hop Traversal: Duy·ªát ƒë·ªì th·ªã qua nhi·ªÅu b∆∞·ªõc\n",
    "   - Relation-aware: Ph√¢n bi·ªát c√°c lo·∫°i quan h·ªá kh√°c nhau\n",
    "   \n",
    "‚úÖ Intelligent Query Processing:\n",
    "   - Entity Extraction: T·ª± ƒë·ªông tr√≠ch xu·∫•t entities t·ª´ c√¢u h·ªèi\n",
    "   - Path Finding: T√¨m ƒë∆∞·ªùng ƒëi gi·ªØa c√°c entities\n",
    "   - Common Connection Detection: Ph√°t hi·ªán ƒëi·ªÉm chung\n",
    "   - Relation Type Filtering: L·ªçc theo lo·∫°i quan h·ªá\n",
    "\n",
    "üìÅ FILES T·∫†O:\n",
    "- kg_chatbot.ipynb (notebook n√†y - v·ªõi GraphRAG implementation)\n",
    "- chatbot_ui.py (Gradio UI)\n",
    "- benchmark_dataset.json (2000+ c√¢u h·ªèi)\n",
    "- graph_out/nodes_unified.csv (2,178 nodes - ƒë√£ s·ª≠a thi·∫øu alumni_of)\n",
    "- graph_out/edges_unified.csv (68,476 edges - ƒë√£ th√™m 24 edges m·ªõi)\n",
    "- fix_missing_alumni.py (script t·ª± ƒë·ªông b·ªï sung alumni_of)\n",
    "- check_missing_alumni.py (script ki·ªÉm tra)\n",
    "\n",
    "üöÄ CH·∫†Y UI:\n",
    "python chatbot_ui.py\n",
    "\n",
    "üéØ KI·∫æN TR√öC GRAPHRAG:\n",
    "Query ‚Üí Entity Extraction ‚Üí Graph Traversal ‚Üí Context Assembly ‚Üí LLM Generation ‚Üí Answer\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ GraphRAG Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ CHATBOT KNOWLEDGE GRAPH V·ªöI GRAPHRAG HO√ÄN CH·ªàNH!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6518cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RAGLLMChatbot:\n",
    "    \"\"\"\n",
    "    RAG + LLM Chatbot: GraphRAG retrieves context, Qwen generates natural response\n",
    "    \n",
    "    Architecture:\n",
    "    Query ‚Üí GraphRAG Context Retrieval ‚Üí Multi-hop Reasoning ‚Üí Qwen LLM ‚Üí Natural Answer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kg: 'KnowledgeGraph', retriever: 'GraphRAGRetriever', \n",
    "                 llm, multi_hop_reasoner: 'MultiHopReasoner' = None):\n",
    "        self.kg = kg\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.multi_hop = multi_hop_reasoner\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def extract_entities(self, query: str) -> List[str]:\n",
    "        \"\"\"Extract person/university names from query\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        # Check against graph nodes\n",
    "        for node in self.kg.graph.nodes():\n",
    "            if node.lower() in query.lower():\n",
    "                entities.append(node)\n",
    "        \n",
    "        return entities[:3]  # Limit to 3 entities\n",
    "    \n",
    "    def chat(self, query: str, use_multi_hop: bool = True, verbose: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Main chat function combining GraphRAG + LLM\n",
    "        \n",
    "        Args:\n",
    "            query: User question in natural language\n",
    "            use_multi_hop: Enable multi-hop reasoning\n",
    "            verbose: Print intermediate steps\n",
    "        \n",
    "        Returns:\n",
    "            {\n",
    "                'question': original query,\n",
    "                'entities': extracted entities,\n",
    "                'graph_context': retrieved context,\n",
    "                'reasoning': multi-hop result if applicable,\n",
    "                'answer': generated answer,\n",
    "                'sources': data sources\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "        result = {\n",
    "            'question': query,\n",
    "            'entities': [],\n",
    "            'graph_context': '',\n",
    "            'reasoning': None,\n",
    "            'answer': '',\n",
    "            'sources': []\n",
    "        }\n",
    "        \n",
    "        # Step 1: Extract entities\n",
    "        entities = self.extract_entities(query)\n",
    "        result['entities'] = entities\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üîç Entities found: {entities}\")\n",
    "        \n",
    "        # Step 2: GraphRAG retrieval\n",
    "        if entities:\n",
    "            context = self.retriever.retrieve_multi_entity_context(entities, hop_depth=2)\n",
    "            result['graph_context'] = context\n",
    "            result['sources'].append('graph_retrieval')\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"üìä GraphRAG Context:\\n{context[:500]}...\")\n",
    "        else:\n",
    "            # Generic graph summary\n",
    "            context = f\"Alumni network graph with {len(self.kg.graph.nodes())} entities and {len(self.kg.graph.edges())} relationships.\"\n",
    "            result['graph_context'] = context\n",
    "        \n",
    "        # Step 3: Multi-hop reasoning (optional)\n",
    "        if use_multi_hop and len(entities) >= 2 and self.multi_hop:\n",
    "            try:\n",
    "                reasoning = self.multi_hop.find_path(entities[0], entities[1], max_hops=3)\n",
    "                result['reasoning'] = reasoning\n",
    "                result['sources'].append('multi_hop_reasoning')\n",
    "                \n",
    "                if verbose and reasoning['connected']:\n",
    "                    print(f\"üîó Multi-hop Path: {reasoning['path']}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Step 4: LLM generation\n",
    "        if hasattr(self.llm, 'generate'):\n",
    "            answer = self.llm.generate(\n",
    "                query=query,\n",
    "                context=result['graph_context'],\n",
    "                reasoning_result=result['reasoning']\n",
    "            )\n",
    "        else:\n",
    "            # SimpleLLM fallback\n",
    "            answer = self.llm.generate_answer(query)\n",
    "        \n",
    "        result['answer'] = answer\n",
    "        \n",
    "        # Store in history\n",
    "        self.chat_history.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def display_result(self, result: Dict):\n",
    "        \"\"\"Pretty print chat result\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"‚ùì Question: {result['question']}\")\n",
    "        print(f\"üìå Entities: {', '.join(result['entities']) if result['entities'] else 'None'}\")\n",
    "        \n",
    "        if result['reasoning'] and result['reasoning'].get('connected'):\n",
    "            print(f\"üîó Path: {' ‚Üí '.join(result['reasoning']['path'])}\")\n",
    "            print(f\"   Hops: {result['reasoning']['hops']}\")\n",
    "        \n",
    "        print(f\"\\nüí¨ Answer:\\n{result['answer']}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Initialize RAG+LLM Chatbot with SimpleLLM (fallback if Qwen not available)\n",
    "print(\"\\nü§ñ Initializing RAG+LLM Chatbot...\")\n",
    "rag_llm_chatbot = RAGLLMChatbot(\n",
    "    kg=kg,\n",
    "    retriever=rag_retriever,\n",
    "    llm=llm,\n",
    "    multi_hop_reasoner=reasoner\n",
    ")\n",
    "print(\"‚úÖ RAG+LLM Chatbot ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d167a",
   "metadata": {},
   "source": [
    "## Demo: RAG+LLM Chatbot with Natural Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG+LLM Chatbot with natural questions\n",
    "print(\"=\"*70)\n",
    "print(\"ü§ñ Testing RAG+LLM Chatbot\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load natural questions\n",
    "import json\n",
    "\n",
    "with open('benchmark_dataset_natural_questions.json', 'r', encoding='utf-8') as f:\n",
    "    natural_questions = json.load(f)\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(natural_questions)} natural questions\")\n",
    "\n",
    "# Test queries from different categories\n",
    "test_queries = [\n",
    "    # Connection question\n",
    "    {\n",
    "        'query': \"C√≥ li√™n quan g√¨ gi·ªØa Bill Gates v√† Mark Zuckerberg?\",\n",
    "        'category': 'connection',\n",
    "        'description': 'Connection between two tech entrepreneurs'\n",
    "    },\n",
    "    # Education lookup\n",
    "    {\n",
    "        'query': \"B·∫°n c√≥ bi·∫øt Bill Gates h·ªçc ·ªü ƒë√¢u kh√¥ng?\",\n",
    "        'category': 'education_lookup',\n",
    "        'description': 'Where did Bill Gates study'\n",
    "    },\n",
    "    # Career lookup  \n",
    "    {\n",
    "        'query': \"Tim Cook l√†m ngh·ªÅ g√¨?\",\n",
    "        'category': 'career_lookup',\n",
    "        'description': 'What is Tim Cook profession'\n",
    "    },\n",
    "    # Inference question (requires LLM reasoning)\n",
    "    {\n",
    "        'query': \"Theo b·∫°n, c√≥ nh·ªØng ƒëi·ªÉm chung n√†o gi·ªØa Steve Jobs v√† Tim Cook?\",\n",
    "        'category': 'inference',\n",
    "        'description': 'Inference about common points'\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "for test_q in test_queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìã Category: {test_q['category']}\")\n",
    "    print(f\"‚ùì Question: {test_q['query']}\")\n",
    "    \n",
    "    result = rag_llm_chatbot.chat(test_q['query'], use_multi_hop=True, verbose=True)\n",
    "    results.append(result)\n",
    "    \n",
    "    rag_llm_chatbot.display_result(result)\n",
    "\n",
    "print(\"\\n‚úÖ Demo completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2e914",
   "metadata": {},
   "source": [
    "## Evaluation: RAG+LLM on Natural Questions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RAG+LLM Chatbot on natural questions dataset\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä Evaluating RAG+LLM Chatbot on Natural Questions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample diverse questions from dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "eval_sample_size = 50\n",
    "eval_questions = random.sample(natural_questions, min(eval_sample_size, len(natural_questions)))\n",
    "\n",
    "print(f\"\\nSampling {len(eval_questions)} questions from {len(natural_questions)} total\")\n",
    "\n",
    "# Categorize eval questions\n",
    "from collections import defaultdict\n",
    "category_results = defaultdict(lambda: {'total': 0, 'answered': 0, 'with_reasoning': 0})\n",
    "\n",
    "eval_start = time.time()\n",
    "\n",
    "for i, q in enumerate(eval_questions):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Progress: {i}/{len(eval_questions)}...\", end='\\r')\n",
    "    \n",
    "    try:\n",
    "        result = rag_llm_chatbot.chat(\n",
    "            q.get('question', ''),\n",
    "            use_multi_hop=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        category = q.get('category', 'unknown')\n",
    "        category_results[category]['total'] += 1\n",
    "        \n",
    "        if result['answer']:\n",
    "            category_results[category]['answered'] += 1\n",
    "        \n",
    "        if result['reasoning'] and result['reasoning'].get('connected'):\n",
    "            category_results[category]['with_reasoning'] += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {i}: {str(e)}\")\n",
    "\n",
    "eval_time = time.time() - eval_start\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà Evaluation Results:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_answered = 0\n",
    "total_questions = 0\n",
    "\n",
    "for category in sorted(category_results.keys()):\n",
    "    stats = category_results[category]\n",
    "    total = stats['total']\n",
    "    answered = stats['answered']\n",
    "    with_reasoning = stats['with_reasoning']\n",
    "    \n",
    "    if total > 0:\n",
    "        answer_rate = (answered / total) * 100\n",
    "        reasoning_rate = (with_reasoning / total) * 100\n",
    "        \n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(f\"  Total: {total}\")\n",
    "        print(f\"  Answered: {answered}/{total} ({answer_rate:.1f}%)\")\n",
    "        print(f\"  With Multi-hop Reasoning: {with_reasoning}/{total} ({reasoning_rate:.1f}%)\")\n",
    "        \n",
    "        total_answered += answered\n",
    "        total_questions += total\n",
    "\n",
    "overall_rate = (total_answered / total_questions * 100) if total_questions > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Overall Answer Rate: {total_answered}/{total_questions} ({overall_rate:.1f}%)\")\n",
    "print(f\"Evaluation Time: {eval_time:.2f}s ({eval_time/len(eval_questions):.3f}s per question)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
