import pandas as pd
import json
import wikipediaapi
from typing import List, Dict
import re

# Khá»Ÿi táº¡o Wikipedia API
wiki = wikipediaapi.Wikipedia(
    language='en',
    user_agent='UniversityAlumniGraph/1.0'
)

def extract_universities_from_wiki(person_name: str) -> List[str]:
    """TrÃ­ch xuáº¥t thÃ´ng tin trÆ°á»ng Ä‘áº¡i há»c tá»« Wikipedia"""
    print(f"  Äang tÃ¬m kiáº¿m: {person_name}...")
    
    page = wiki.page(person_name)
    
    if not page.exists():
        print(f"    âš  KhÃ´ng tÃ¬m tháº¥y trang Wikipedia")
        return []
    
    # Láº¥y ná»™i dung vÄƒn báº£n
    text = page.text
    
    # CÃ¡c tá»« khÃ³a tÃ¬m kiáº¿m
    university_keywords = [
        r'University of ([A-Z][A-Za-z\s]+)',
        r'([A-Z][A-Za-z\s]+) University',
        r'attended ([A-Z][A-Za-z\s]+ University)',
        r'studied at ([A-Z][A-Za-z\s]+ University)',
        r'graduated from ([A-Z][A-Za-z\s]+ University)',
        r'alma mater[:\s]+([A-Z][A-Za-z\s]+ University)',
        r'educated at ([A-Z][A-Za-z\s]+ University)',
        r'College of ([A-Z][A-Za-z\s]+)',
        r'([A-Z][A-Za-z\s]+) College',
        r'MIT',
        r'Stanford',
        r'Harvard',
        r'Princeton',
        r'Yale',
        r'Oxford',
        r'Cambridge'
    ]
    
    found_unis = set()
    
    for pattern in university_keywords:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                match = match[0]
            
            # LÃ m sáº¡ch tÃªn trÆ°á»ng
            match = match.strip()
            if len(match) > 5:  # Bá» qua tÃªn quÃ¡ ngáº¯n
                found_unis.add(match)
    
    return list(found_unis)

# Mapping tÃªn tiáº¿ng Anh -> tÃªn tiáº¿ng Viá»‡t
university_mapping = {
    'Harvard': 'Äáº¡i há»c Harvard',
    'Stanford': 'Äáº¡i há»c Stanford',
    'MIT': 'Viá»‡n CÃ´ng nghá»‡ Massachusetts',
    'Oxford': 'Äáº¡i há»c Oxford',
    'Cambridge': 'Äáº¡i há»c Cambridge',
    'Princeton': 'Äáº¡i há»c Princeton',
    'Yale': 'Äáº¡i há»c Yale',
    'Pennsylvania': 'Äáº¡i há»c Pennsylvania',
    'Columbia': 'Äáº¡i há»c Columbia',
    'Chicago': 'Äáº¡i há»c Chicago',
    'Michigan': 'Äáº¡i há»c Michigan',
    'California': 'Äáº¡i há»c California',
    'Berkeley': 'Äáº¡i há»c California, Berkeley',
    'Duke': 'Äáº¡i há»c Duke',
    'Cornell': 'Äáº¡i há»c Cornell',
    'Northwestern': 'Äáº¡i há»c Northwestern',
    'Dartmouth': 'Äáº¡i há»c Dartmouth',
    'Brown': 'Äáº¡i há»c Brown',
    'Wharton': 'TrÆ°á»ng Kinh doanh Wharton',
    'Delhi': 'Äáº¡i há»c Delhi',
    'Manipal': 'Äáº¡i há»c Manipal',
    'Wisconsin': 'Äáº¡i há»c Wisconsin',
    'Illinois': 'Äáº¡i há»c Illinois',
    'Waterloo': 'Äáº¡i há»c Waterloo',
    'Toronto': 'Äáº¡i há»c Toronto',
    'McGill': 'Äáº¡i há»c McGill',
    'London': 'Äáº¡i há»c London',
    'Sorbonne': 'Äáº¡i há»c Sorbonne',
    'Berlin': 'Äáº¡i há»c Berlin',
    'Munich': 'Äáº¡i há»c Munich',
    'Tokyo': 'Äáº¡i há»c Tokyo',
    'Kyoto': 'Äáº¡i há»c Kyoto',
    'Seoul': 'Äáº¡i há»c Seoul',
    'Peking': 'Äáº¡i há»c Báº¯c Kinh',
    'Tsinghua': 'Äáº¡i há»c Thanh Hoa',
}

def normalize_university_name(uni_name: str) -> str:
    """Chuáº©n hÃ³a tÃªn trÆ°á»ng"""
    # Kiá»ƒm tra mapping
    for eng, vie in university_mapping.items():
        if eng in uni_name:
            return vie
    
    # Náº¿u khÃ´ng cÃ³ mapping, thÃªm "Äáº¡i há»c" náº¿u chÆ°a cÃ³
    if not uni_name.startswith('Äáº¡i há»c') and not uni_name.startswith('Viá»‡n') and not uni_name.startswith('TrÆ°á»ng'):
        return f'Äáº¡i há»c {uni_name}'
    
    return uni_name

# ThÃ´ng tin trÆ°á»ng há»c Ä‘Ã£ biáº¿t (tá»« nguá»“n tin cáº­y)
known_alumni = {
    'Bill Gates': ['Microsoft', 'Äáº¡i há»c Harvard'],  # dropout
    'Mark Zuckerberg': ['Äáº¡i há»c Harvard'],  # dropout
    'Elon Musk': ['Äáº¡i há»c Pennsylvania', 'Äáº¡i há»c Stanford'],  # dropout Stanford
    'Jeff Bezos': ['Äáº¡i há»c Princeton'],
    'Sundar Pichai': ['Äáº¡i há»c Stanford', 'Viá»‡n CÃ´ng nghá»‡ Massachusetts', 'Viá»‡n CÃ´ng nghá»‡ áº¤n Äá»™ Kharagpur'],
    'Satya Nadella': ['Äáº¡i há»c Chicago', 'Äáº¡i há»c Wisconsin-Milwaukee', 'Viá»‡n CÃ´ng nghá»‡ Manipal'],
    'Tim Cook': ['Äáº¡i há»c Auburn', 'Äáº¡i há»c Duke'],
    'Peter Thiel': ['Äáº¡i há»c Stanford'],
    'Sheryl Sandberg': ['Äáº¡i há»c Harvard'],
    'Nancy Pelosi': ['Äáº¡i há»c Trinity Washington'],
    'Taylor Swift': ['Äáº¡i há»c New York'],  # honorary degree
    'Malala Yousafzai': ['Äáº¡i há»c Oxford'],
    'Michelangelo': ['Äáº¡i há»c Florence'],  # historical, approximate
    'Helmut Schmidt': ['Äáº¡i há»c Hamburg'],
    'Michel Barnier': ['Äáº¡i há»c Paris II PanthÃ©on-Assas'],
    'Kaja Kallas': ['Äáº¡i há»c Tartu'],
    'Jacques Chaban-Delmas': ['Äáº¡i há»c Paris']
}

print("=" * 80)
print("Báº®T Äáº¦U Bá»” SUNG THÃ”NG TIN TRÆ¯á»œNG Há»ŒC")
print("=" * 80)

# Äá»c danh sÃ¡ch person thiáº¿u alumni_of
with open('persons_missing_alumni.json', 'r', encoding='utf-8') as f:
    missing_persons = json.load(f)

# Äá»c dá»¯ liá»‡u hiá»‡n táº¡i
nodes = pd.read_csv('graph_out/nodes_unified.csv')
edges = pd.read_csv('graph_out/edges_unified.csv')

# Láº¥y danh sÃ¡ch cÃ¡c trÆ°á»ng Ä‘Ã£ cÃ³
existing_universities = set(nodes[nodes['type'] == 'university']['title'].tolist())

new_edges = []
new_universities = []

for person in missing_persons:
    print(f"\n[{person}]")
    
    # Æ¯u tiÃªn dÃ¹ng dá»¯ liá»‡u Ä‘Ã£ biáº¿t
    if person in known_alumni:
        unis = known_alumni[person]
        print(f"  âœ“ Sá»­ dá»¥ng dá»¯ liá»‡u cÃ³ sáºµn: {unis}")
    else:
        # Thá»­ tra Wikipedia
        unis_raw = extract_universities_from_wiki(person)
        unis = [normalize_university_name(u) for u in unis_raw]
        print(f"  âœ“ TÃ¬m tháº¥y tá»« Wikipedia: {unis}")
    
    # ThÃªm edges vÃ  universities má»›i
    for uni in unis:
        # ThÃªm university node náº¿u chÆ°a cÃ³
        if uni not in existing_universities:
            new_universities.append({
                'id': uni,
                'title': uni,
                'type': 'university'
            })
            existing_universities.add(uni)
            print(f"    + ThÃªm trÆ°á»ng má»›i: {uni}")
        
        # ThÃªm edge alumni_of
        new_edges.append({
            'from': person,
            'to': uni,
            'type': 'alumni_of',
            'weight': 1
        })
        print(f"    + ThÃªm edge: {person} --[alumni_of]--> {uni}")

print("\n" + "=" * 80)
print("Káº¾T QUáº¢")
print("=" * 80)
print(f"âœ“ ÄÃ£ thÃªm {len(new_edges)} edges má»›i (alumni_of)")
print(f"âœ“ ÄÃ£ thÃªm {len(new_universities)} universities má»›i")

# Cáº­p nháº­t dá»¯ liá»‡u
if new_universities:
    new_unis_df = pd.DataFrame(new_universities)
    nodes = pd.concat([nodes, new_unis_df], ignore_index=True)
    nodes.to_csv('graph_out/nodes_unified.csv', index=False, encoding='utf-8')
    print(f"\nâœ“ ÄÃ£ cáº­p nháº­t nodes_unified.csv")

if new_edges:
    new_edges_df = pd.DataFrame(new_edges)
    edges = pd.concat([edges, new_edges_df], ignore_index=True)
    edges.to_csv('graph_out/edges_unified.csv', index=False, encoding='utf-8')
    print(f"âœ“ ÄÃ£ cáº­p nháº­t edges_unified.csv")

# Kiá»ƒm tra láº¡i
alumni_edges = edges[edges['type'] == 'alumni_of']
persons_with_alumni = set(alumni_edges['from'].unique())
all_persons = nodes[nodes['type'] == 'person']['id'].tolist()
still_missing = set(all_persons) - persons_with_alumni

print(f"\nğŸ“Š THá»NG KÃŠ SAU KHI Sá»¬A:")
print(f"  - Tá»•ng person: {len(all_persons)}")
print(f"  - Person cÃ³ alumni_of: {len(persons_with_alumni)}")
print(f"  - Person váº«n cÃ²n thiáº¿u: {len(still_missing)}")

if len(still_missing) == 0:
    print("\nğŸ‰ HOÃ€N THÃ€NH! Táº¥t cáº£ person Ä‘á»u Ä‘Ã£ cÃ³ alumni_of")
else:
    print(f"\nâš  CÃ²n {len(still_missing)} person chÆ°a cÃ³ alumni_of: {list(still_missing)}")
