# -*- coding: utf-8 -*-
"""
run_pipeline_clean.py
- Step 1: Crawl links/info cho ROOTS (append n·∫øu thi·∫øu)
- Step 2: Build seeds/edges multi-root (√©p ch·∫°y l·∫°i n·∫øu c·∫ßn)
- Step 3: BFS m·ªü r·ªông
- Step 4: Enrich + export node_details & edges (KH√îNG LINKS_TO)
- Step 5: Clean output ‚Äî t·∫≠p k·∫øt qu·∫£ cu·ªëi c√πng (edges, node details, graph.json, node props) ph·ª•c v·ª• import v√† tr·ª±c quan h√≥a.
"""

import os, sys, subprocess, json, tempfile, shutil
import pandas as pd
from utils_wiki import normalize

# =============================
# ===== GLOBAL CONFIG =========
# =============================
OUT = "graph_out"
ROOTS = [
  # People
  "Barack Obama","Elon Musk","Mark Zuckerberg","Bill Gates","Jeff Bezos",
  "Satya Nadella","Sundar Pichai","Tim Cook","Sheryl Sandberg","Peter Thiel",
  "Malala Yousafzai","Angela Merkel","Emmanuel Macron","Taylor Swift",
  # Universities
  "ƒê·∫°i h·ªçc Harvard", "ƒê·∫°i h·ªçc Stanford", "ƒê·∫°i h·ªçc Oxford",
  "ƒê·∫°i h·ªçc Cambridge", "Vi·ªán C√¥ng ngh·ªá Massachusetts", "ƒê·∫°i h·ªçc Yale"
]

FORCE_CLEAN = True      # X√≥a s·∫°ch graph_out m·ªói l·∫ßn ch·∫°y
FORCE_STEP2 = True      # √âp Step2 ch·∫°y l·∫°i d√π file ƒë√£ t·ªìn t·∫°i

# (Tu·ª≥ ch·ªçn) th√¥ng s·ªë Step 4
STEP4_WORKERS = os.cpu_count() or 8
STEP4_SLEEP   = "0.06"
STEP4_TIMEOUT = "6.0"

# Ch·ªâ gi·ªØ l·∫°i c√°c file n√†y sau Step 5
KEEP_FILES = {
    # edges
    "edges_alumni_pu.csv",
    "edges_mentions_pp.csv",
    "edges_mentions_pu.csv",
    "edges_shared_uni_pp.csv",
    "edges_uni_mentions_p.csv",
    "edges_uni_mentions_u.csv",
    # graph + node details
    "graph.json",
    "node_details.csv",
    "node_details.json",
    # node props
    "nodes_persons_props.csv",
    "nodes_universities_props.csv",
}

# =============================
# ====== ENV SETTINGS =========
# =============================
ENV = os.environ.copy()
ENV["PYTHONUNBUFFERED"] = "1"

def slug(s: str):
    import re
    s = re.sub(r"\s+", "_", s.strip())
    return re.sub(r"[^\w\-\.]", "", s, flags=re.UNICODE)

def load_links_df():
    fp = os.path.join(OUT, "links.csv")
    if os.path.exists(fp):
        try:
            return pd.read_csv(fp)
        except Exception:
            return pd.DataFrame(columns=["source_title","target_title"])
    return pd.DataFrame(columns=["source_title","target_title"])

def load_info_list():
    fp = os.path.join(OUT, "info.json")
    if os.path.exists(fp):
        try:
            data = json.load(open(fp, encoding="utf-8"))
            if isinstance(data, list): return data
            if isinstance(data, dict): return [data]
        except Exception:
            pass
    return []

def info_has_root(info_list, norm_title):
    for it in info_list:
        if normalize(it.get("title","")) == norm_title:
            return True
    return False

def safe_concat(lst, cols):
    return (pd.concat(lst, ignore_index=True) if lst else pd.DataFrame(columns=cols)).drop_duplicates()

def run_cmd(cmd, desc=None):
    if desc:
        print(desc)
    print("  $", " ".join(map(str, cmd)))
    subprocess.run(cmd, check=True, env=ENV)

# =============================
# ====== START PIPELINE =======
# =============================

print("=== CLEAN PIPELINE: Step1 ‚Üí Step2 ‚Üí Step3 ‚Üí Step4 ‚Üí Step5 ===")

# üßπ CLEAN OUTPUT
if FORCE_CLEAN and os.path.exists(OUT):
    print(f"\nüßπ X√≥a s·∫°ch th∆∞ m·ª•c {OUT}/ ƒë·ªÉ ch·∫°y l·∫°i t·ª´ ƒë·∫ßu ...")
    shutil.rmtree(OUT, ignore_errors=True)
os.makedirs(OUT, exist_ok=True)

# ---------- PHASE 1 ----------
print("\n[PHASE 1/5] Step 1 ‚Äî Crawl links & info (append n·∫øu thi·∫øu)")

links_df = load_links_df()
info_list = load_info_list()

for i, title in enumerate(ROOTS, 1):
    norm = normalize(title)
    has_links = False
    if not links_df.empty and "source_title" in links_df.columns:
        has_links = (links_df["source_title"].map(str) == norm).any()
    has_info = info_has_root(info_list, norm)
    if has_links and has_info:
        print(f"  [{i}/{len(ROOTS)}] ‚è≠Ô∏è Skip: '{title}' ƒë√£ c√≥ trong links.csv & info.json")
        continue

    print(f"  [{i}/{len(ROOTS)}] step1: {title} (crawl + append)")
    run_cmd(
        [sys.executable, "-u", "step1_single_node_links.py", "--title", title, "--outdir", OUT]
    )
    links_df = load_links_df()
    info_list = load_info_list()

print(f"  ‚úì links.csv rows = {links_df.shape[0]} | info.json roots = {len(info_list)}")

# ---------- PHASE 2 ----------
print("\n[PHASE 2/5] Step 2 ‚Äî Build seeds & edges (multi-root)")

all_seeds = []
all_person_edges = []
all_edu_edges = []
all_root_nodes = []

for i, title in enumerate(ROOTS, 1):
    norm = normalize(title)
    root_links = links_df[links_df["source_title"].map(str) == norm]
    if root_links.empty:
        print(f"  [{i}/{len(ROOTS)}] ‚ö†Ô∏è Kh√¥ng c√≥ links cho '{title}' ‚Üí b·ªè qua Step2")
        continue

    tmpdir = tempfile.mkdtemp(prefix=f"_tmp_{slug(title)}_", dir=OUT)
    tmp_links_fp = os.path.join(tmpdir, "links.csv")
    root_links.to_csv(tmp_links_fp, index=False, encoding="utf-8")

    is_uni = any(k.lower() in title.lower() for k in ["ƒë·∫°i h·ªçc","university","h·ªçc vi·ªán","institute","college"])
    print(f"  [{i}/{len(ROOTS)}] Step2: {title} ‚Üí {'UNI' if is_uni else 'PERSON'}")

    args2 = [sys.executable, "-u", "step2_build_seeds.py",
             "--links-csv", tmp_links_fp, "--outdir", tmpdir, "--dedupe"]
    if is_uni: args2 += ["--assume-university"]
    else: args2 += ["--include-root-seed"]

    run_cmd(args2 + ["--progress-every", "50"])

    for fn in ("seeds.csv", "person_edges.csv", "edu_edges.csv", "root_nodes.csv"):
        fp = os.path.join(tmpdir, fn)
        if os.path.exists(fp):
            df = pd.read_csv(fp)
            if fn == "seeds.csv": all_seeds.append(df)
            elif fn == "person_edges.csv": all_person_edges.append(df)
            elif fn == "edu_edges.csv": all_edu_edges.append(df)
            elif fn == "root_nodes.csv": all_root_nodes.append(df)

    shutil.rmtree(tmpdir, ignore_errors=True)

# g·ªôp & ghi
seeds_df = safe_concat(all_seeds, ["person_title"])
person_edges_df = safe_concat(all_person_edges, ["src_root","dst_person","relation"])
edu_edges_df = safe_concat(all_edu_edges, ["src_university","dst_person","relation","year"])
root_nodes_df = safe_concat(all_root_nodes, ["title","type"])

seeds_fp = os.path.join(OUT, "seeds.csv")
person_edges_fp = os.path.join(OUT, "person_edges.csv")
edu_edges_fp = os.path.join(OUT, "edu_edges.csv")
root_nodes_fp = os.path.join(OUT, "root_nodes.csv")

seeds_df.to_csv(seeds_fp, index=False, encoding="utf-8")
person_edges_df.to_csv(person_edges_fp, index=False, encoding="utf-8")
edu_edges_df.to_csv(edu_edges_fp, index=False, encoding="utf-8")
root_nodes_df.to_csv(root_nodes_fp, index=False, encoding="utf-8")

print(f"  ‚úì seeds={len(seeds_df)} | person_edges={len(person_edges_df)} | edu_edges={len(edu_edges_df)} | roots={len(root_nodes_df)}")

# rescue n·∫øu seeds tr·ªëng
if seeds_df.empty and not person_edges_df.empty:
    dst = (person_edges_df.loc[person_edges_df["relation"]=="LINK_FROM_START","dst_person"]
           .dropna().drop_duplicates())
    if not dst.empty:
        pd.DataFrame({"person_title": dst}).to_csv(seeds_fp, index=False, encoding="utf-8")
        print(f"  üîÅ rescued seeds.csv from person_edges: {len(dst)} rows")

# rescue root_nodes n·∫øu thi·∫øu
if root_nodes_df.empty:
    rows = []
    for info in load_info_list():
        t = info.get("title")
        if not t: continue
        tl = t.lower()
        ty = "university" if any(k in tl for k in ["ƒë·∫°i h·ªçc","university","h·ªçc vi·ªán","institute","college"]) else "person"
        rows.append((t, ty))
    if rows:
        pd.DataFrame(rows, columns=["title","type"]).to_csv(root_nodes_fp, index=False, encoding="utf-8")
        print(f"  üîÅ rescued root_nodes.csv with {len(rows)} rows")

# ---------- PHASE 3 ----------
print("\n[PHASE 3/5] Step 3 ‚Äî BFS m·ªü r·ªông ‚Üí graph_out/")

step3_cmd = [
    sys.executable, "-u", "step3_bfs_expand.py",
    "--seeds", os.path.join(OUT, "seeds.csv"),
    "--config", "config_example.json",
    "--outdir", OUT,
    "--checkpoint-every", "500",
    "--flush-every", "500",
    "--roots-csv", os.path.join(OUT, "root_nodes.csv"),
    "--expand-from-university",
    "--uni-candidate-cap", "200",
    "--info-json", os.path.join(OUT, "info.json"),
]
run_cmd(step3_cmd, desc=None)

print("\n  ‚úì Step 3 ho√†n t·∫•t")

# ---------- PHASE 4 ----------
print("\n[PHASE 4/5] Step 4 ‚Äî Enrich + export node_details & edges (no LINKS_TO)")

step4_cmd = [
    sys.executable, "-u", "step4_enrich_full.py",
    "--outdir", OUT,
    "--workers", str(STEP4_WORKERS),
    "--sleep", STEP4_SLEEP,
    "--http-timeout", STEP4_TIMEOUT,
    # n·∫øu c·∫ßn gi·ªõi h·∫°n ƒë·ªÉ test, th√™m:
    # "--limit-persons","xxx","--limit-universities","xxx"
]
run_cmd(step4_cmd)

print("  ‚úì Step 4 ƒë√£ xu·∫•t:")
for fn in [
    "edges_alumni_pu.csv",
    "edges_mentions_pp.csv", "edges_mentions_pu.csv",
    "edges_shared_uni_pp.csv", "edges_uni_mentions_p.csv", "edges_uni_mentions_u.csv",
    "node_details.csv", "node_details.json",
    "nodes_persons_props.csv", "nodes_universities_props.csv",
]:
    p = os.path.join(OUT, fn)
    print(f"    - {fn} {'(OK)' if os.path.exists(p) else '(MISSING!)'}")

# ---------- PHASE 5 ----------
print("\n[PHASE 5/5] Clean output ‚Äî ch·ªâ gi·ªØ file c·∫ßn thi·∫øt ƒë·ªÉ import Neo4j")

run_cmd([sys.executable, "-u", "step5_clean.py", "--outdir", OUT])

print("\n‚úÖ DONE. C√°c file d·ªØ li·ªáu cu·ªëi c√πng trong graph_out/ (edges, node details, graph.json, node props).")
