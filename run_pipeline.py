# -*- coding: utf-8 -*-
import os, sys, subprocess, json, tempfile, shutil
import pandas as pd
from utils_wiki import normalize

OUT = "graph_out"
os.makedirs(OUT, exist_ok=True)

# ======= ROOTS (tr∆∞·ªùng + ng∆∞·ªùi) =======
ROOTS = [
    "Mark Zuckerberg",
    # c√≥ th·ªÉ th√™m nhi·ªÅu root kh√°c ·ªü ƒë√¢y
]

# √©p log realtime
ENV = os.environ.copy()
ENV["PYTHONUNBUFFERED"] = "1"

def slug(s: str) -> str:
    import re
    s = re.sub(r"\s+", "_", s.strip())
    return re.sub(r"[^\w\-\.]", "", s, flags=re.UNICODE)

def load_links_df():
    fp = os.path.join(OUT, "links.csv")
    if os.path.exists(fp):
        try:
            return pd.read_csv(fp)
        except Exception:
            return pd.DataFrame(columns=["source_title","target_title"])
    return pd.DataFrame(columns=["source_title","target_title"])

def load_info_list():
    fp = os.path.join(OUT, "info.json")
    if os.path.exists(fp):
        try:
            data = json.load(open(fp, encoding="utf-8"))
            if isinstance(data, list): return data
            if isinstance(data, dict): return [data]
        except Exception:
            pass
    return []

def info_has_root(info_list, norm_title):
    for it in info_list:
        if normalize(it.get("title","")) == norm_title:
            return True
    return False

print("=== PIPELINE (append 1 links.csv + 1 info.json + 1 seeds/person_edges/edu_edges) ‚Üí graph_out/ ===")

# ---------- PHASE 1: STEP 1 APPEND (idempotent) ----------
print("\n[PHASE 1/3] Step 1 (append n·∫øu thi·∫øu)")
links_df = load_links_df()
info_list = load_info_list()

for i, title in enumerate(ROOTS, 1):
    norm = normalize(title)
    has_links = False
    if not links_df.empty and "source_title" in links_df.columns:
        has_links = (links_df["source_title"].map(str) == norm).any()
    has_info = info_has_root(info_list, norm)
    if has_links and has_info:
        print(f"  [{i}/{len(ROOTS)}] ‚è≠Ô∏è Skip step1: '{title}' ƒë√£ c√≥ trong links.csv & info.json")
        continue

    print(f"  [{i}/{len(ROOTS)}] step1: {title} (append)")
    subprocess.run(
        [sys.executable, "-u", "step1_single_node_links.py",
         "--title", title, "--outdir", OUT],  # step1 s·∫Ω t·ª± append/dedupe theo code b·∫°n ƒë√£ ch·ªânh
        check=True, env=ENV
    )
    # reload sau khi append
    links_df = load_links_df()
    info_list = load_info_list()

print(f"  ‚úì links.csv rows = {links_df.shape[0]} | info.json roots = {len(info_list)}")

# ---------- PHASE 2: STEP 2 PER-ROOT (kh√¥ng crawl l·∫°i) ----------
print("\n[PHASE 2/3] Step 2 cho t·ª´ng root, d·ª±a tr√™n links.csv ƒë√£ g·ªôp (NO re-crawl)")

# b·ªô nh·ªõ g·ªôp
all_seeds = []
all_person_edges = []
all_edu_edges = []

for i, title in enumerate(ROOTS, 1):
    norm = normalize(title)
    # l·ªçc links cho ri√™ng root n√†y
    root_links = links_df[links_df["source_title"].map(str) == norm]
    if root_links.empty:
        print(f"  [{i}/{len(ROOTS)}] ‚ö†Ô∏è Kh√¥ng c√≥ links cho '{title}' trong links.csv ‚Üí b·ªè qua Step2 root n√†y")
        continue

    # n·∫øu person_edges.csv ƒë√£ c√≥ LINK_FROM_START cho root n√†y ‚Üí b·ªè qua
    person_edges_fp = os.path.join(OUT, "person_edges.csv")
    skip_step2 = False
    if os.path.exists(person_edges_fp):
        try:
            pe = pd.read_csv(person_edges_fp)
            has_link = (not pe.empty and
                        "relation" in pe.columns and "src_root" in pe.columns and
                        ((pe["relation"]=="LINK_FROM_START") & (pe["src_root"].map(str)==norm)).any())
            if has_link:
                # ch·ªâ skip n·∫øu seeds ƒë√£ ch·ª©a √çT NH·∫§T M·ªòT dst_person t∆∞∆°ng ·ª©ng root n√†y
                seeds_fp_global = os.path.join(OUT, "seeds.csv")
                has_seeds = False
                if os.path.exists(seeds_fp_global):
                    s = pd.read_csv(seeds_fp_global)
                    if not s.empty and "person_title" in s.columns:
                        dst = pe.loc[(pe["relation"]=="LINK_FROM_START") &
                                    (pe["src_root"].map(str)==norm), "dst_person"].dropna().unique()
                        if len(dst) > 0:
                            has_seeds = s["person_title"].isin(dst).any()
                if has_seeds:
                    print(f"  [{i}/{len(ROOTS)}] ‚è≠Ô∏è Skip step2: '{title}' ƒë√£ c√≥ LINK_FROM_START v√† seeds")
                    skip_step2 = True
                else:
                    print(f"  [{i}/{len(ROOTS)}] ‚ö†Ô∏è C√≥ LINK_FROM_START nh∆∞ng thi·∫øu seeds ‚Üí s·∫Ω ch·∫°y l·∫°i Step2")
        except Exception:
            pass

    if skip_step2:
        continue

    # ghi m·ªôt CSV t·∫°m ch·ªâ ch·ª©a links c·ªßa root n√†y ƒë·ªÉ step2 x·ª≠ l√Ω
    tmpdir = tempfile.mkdtemp(prefix=f"_tmp_{slug(title)}_", dir=OUT)
    tmp_links_fp = os.path.join(tmpdir, "links.csv")
    root_links.to_csv(tmp_links_fp, index=False, encoding="utf-8")

    # ƒëo√°n lo·∫°i root (tr∆∞·ªùng hay ng∆∞·ªùi) theo t√™n (heuristic nh·∫π)
    is_uni = any(k.lower() in title.lower() for k in ["ƒë·∫°i h·ªçc","university","h·ªçc vi·ªán","institute","college"])

    print(f"  [{i}/{len(ROOTS)}] step2: {title} ‚Üí {'UNI' if is_uni else 'PERSON'}")
    args2 = [sys.executable, "-u", "step2_build_seeds.py", "--links-csv", tmp_links_fp, "--outdir", tmpdir]
    if is_uni:
        args2 += ["--assume-university"]
    else:
        args2 += ["--include-root-seed"]

    subprocess.run(args2 + ["--progress-every", "50"], check=True, env=ENV)

    # ƒë·ªçc k·∫øt qu·∫£ t·∫°m v√† g·ªôp v√†o b·ªô nh·ªõ
    fp_seeds = os.path.join(tmpdir, "seeds.csv")
    fp_pe    = os.path.join(tmpdir, "person_edges.csv")
    fp_ee    = os.path.join(tmpdir, "edu_edges.csv")

    if os.path.exists(fp_seeds):
        all_seeds.append(pd.read_csv(fp_seeds))
    if os.path.exists(fp_pe):
        all_person_edges.append(pd.read_csv(fp_pe))
    if os.path.exists(fp_ee):
        all_edu_edges.append(pd.read_csv(fp_ee))

    # d·ªçn tmp
    shutil.rmtree(tmpdir, ignore_errors=True)

# g·ªôp & dedupe ‚Üí ghi ra graph_out
seeds_df = (pd.concat(all_seeds, ignore_index=True) if all_seeds else pd.DataFrame(columns=["person_title"])).drop_duplicates()
person_edges_df = (pd.concat(all_person_edges, ignore_index=True) if all_person_edges else pd.DataFrame(columns=["src_root","dst_person","relation"])).drop_duplicates()
edu_edges_df = (pd.concat(all_edu_edges, ignore_index=True) if all_edu_edges else pd.DataFrame(columns=["src_university","dst_person","relation","year"])).drop_duplicates()

# (tu·ª≥ ch·ªçn) n·∫øu mu·ªën gi·ªõi h·∫°n 10 seed ban ƒë·∫ßu:
# seeds_df = seeds_df.head(10)

seeds_fp = os.path.join(OUT, "seeds.csv")
person_edges_fp = os.path.join(OUT, "person_edges.csv")
edu_edges_fp = os.path.join(OUT, "edu_edges.csv")

seeds_df.to_csv(seeds_fp, index=False, encoding="utf-8")
person_edges_df.to_csv(person_edges_fp, index=False, encoding="utf-8")
edu_edges_df.to_csv(edu_edges_fp, index=False, encoding="utf-8")

print(f"  ‚úì seeds.csv rows = {seeds_df.shape[0]} | person_edges.csv = {person_edges_df.shape[0]} | edu_edges.csv = {edu_edges_df.shape[0]}")
# üîÅ Rescue: n·∫øu seeds r·ªóng nh∆∞ng ƒë√£ c√≥ LINK_FROM_START ‚Üí t·∫°o seeds t·ª´ person_edges
if seeds_df.empty and not person_edges_df.empty:
    dst = (person_edges_df.loc[person_edges_df["relation"]=="LINK_FROM_START", "dst_person"]
            .dropna().drop_duplicates())
    if not dst.empty:
        seeds_df = pd.DataFrame({"person_title": dst})
        seeds_df.to_csv(seeds_fp, index=False, encoding="utf-8")
        print(f"  üîÅ rescued seeds.csv from person_edges: {len(dst)} rows")

root_nodes_fp = os.path.join(OUT, "root_nodes.csv")
if not os.path.exists(root_nodes_fp) or os.path.getsize(root_nodes_fp)==0:
    rows = []
    for info in load_info_list():  # b·∫°n ƒë√£ c√≥ helper n√†y ·ªü ƒë·∫ßu file
        t = info.get("title")
        if not t: continue
        tl = t.lower()
        ty = "university" if any(k in tl for k in ["ƒë·∫°i h·ªçc","university","h·ªçc vi·ªán","institute","college"]) else "person"
        rows.append((t, ty))
    if rows:
        pd.DataFrame(rows, columns=["title","type"]).drop_duplicates().to_csv(root_nodes_fp, index=False, encoding="utf-8")
        print(f"  üîÅ rescued root_nodes.csv with {len(rows)} rows")

# ---------- PHASE 3: STEP 3 (BFS) ----------
print("\n[PHASE 3/3] Step 3 (BFS m·ªü r·ªông) ‚Üí graph_out/")

# d√πng info.json ƒë·ªÉ t·ª± ƒë·ªông expand-from-university (n·∫øu b·∫°n ƒë√£ th√™m c·ªù n√†y trong step3)
step3_cmd = [
    sys.executable, "-u", "step3_bfs_expand.py",
    "--seeds", seeds_fp,
    "--config", "config_example.json",
    "--outdir", OUT,
    "--checkpoint-every", "500",
    "--flush-every", "500",
    "--roots-csv", os.path.join(OUT, "root_nodes.csv"),
]
step3_cmd += ["--expand-from-university", "--info-json", os.path.join(OUT, "info.json")]


subprocess.run(step3_cmd, check=True, env=ENV)

print("\n‚úÖ DONE. T·∫•t c·∫£ file cu·ªëi c√πng n·∫±m trong graph_out/:")
print("   - links.csv, info.json, seeds.csv, person_edges.csv, edu_edges.csv, nodes_*.csv, edges_*.csv, graph.json")
