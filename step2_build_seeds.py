# -*- coding: utf-8 -*-
import csv, argparse, json, os, re
from collections import defaultdict
from utils_wiki import (
    fetch_parse_html, soup_from_html, is_person_page,
    extract_person_education, normalize
)

# ========== tqdm ==========
try:
    from tqdm import tqdm
    HAS_TQDM = True
except Exception:
    HAS_TQDM = False


# ========== IO helpers ==========
def load_candidates_grouped(links_csv, filter_title=None):
    """
    ƒê·ªçc links.csv (g·ªôp) v√† gom c√°c target theo t·ª´ng source_title.
    N·∫øu filter_title != None th√¨ ch·ªâ l·∫•y ƒë√∫ng nh√≥m ƒë√≥.
    Tr·∫£ v·ªÅ: dict {root_title_norm: [target_title_raw,...]}
    """
    groups = defaultdict(list)
    with open(links_csv, "r", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            s = normalize(r["source_title"])
            t = r["target_title"]
            if filter_title is not None and normalize(filter_title) != s:
                continue
            groups[s].append(t)
    return groups


def ensure_parent(path):
    os.makedirs(os.path.dirname(path), exist_ok=True)


def open_csv_writer(path, header, append=False):
    mode = "a" if append and os.path.exists(path) else "w"
    f = open(path, mode, encoding="utf-8", newline="")
    w = csv.writer(f)
    if mode == "w":
        w.writerow(header)
    return f, w


# ========== DEDUPE helpers ==========
def _norm_tuple(*cols):
    """T·∫°o kh√≥a dedupe ƒë√£ chu·∫©n ho√° (lower/strip/normalize cho chu·ªói)."""
    return tuple(normalize(x) if isinstance(x, str) else x for x in cols)


def load_existing_pairs_norm(path, cols):
    """
    ƒê·ªçc file hi·ªán c√≥ v√† tr·∫£ v·ªÅ set kh√≥a CHU·∫®N HO√Å theo danh s√°ch c·ªôt.
    D√πng cho so kh·ªõp dedupe theo _norm_tuple.
    """
    s = set()
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            rdr = csv.DictReader(f)
            for r in rdr:
                s.add(_norm_tuple(*(r[c] for c in cols)))
    return s


# ========== Heuristics: x√°c ƒë·ªãnh 'university' ==========
def looks_like_university_title(txt: str) -> bool:
    if not txt:
        return False
    t = (txt or "").strip().lower()

    keywords = [
        "ƒë·∫°i h·ªçc", "h·ªçc vi·ªán", "tr∆∞·ªùng ", " vi·ªán ", "khoa ",
        "university", "college", "institute", "academy", "faculty", "school", "law school", "business school"
    ]
    return any(k in t for k in keywords)


def looks_like_university_infobox(soup) -> bool:
    """
    Suy lu·∫≠n 'trang t·ªï ch·ª©c/ƒë·∫°i h·ªçc' d·ª±a tr√™n c√°c kh√≥a th∆∞·ªùng g·∫∑p trong infobox.
    Ph√¢n bi·ªát v·ªõi person khi ti√™u ƒë·ªÅ m∆° h·ªì.
    """
    try:
        box = soup.find("table", {"class": re.compile(r"\binfobox\b", re.I)})
        if not box:
            return False
        keys = set()
        for tr in box.find_all("tr"):
            th = tr.find("th")
            if th:
                k = th.get_text(strip=True)
                if k:
                    keys.add(k.lower())
        orgish = {
            "lo·∫°i h√¨nh", "th√†nh l·∫≠p", "tr·ª• s·ªü", "s·ªë l∆∞·ª£ng sinh vi√™n", "khoa",
            "trang web", "hi·ªáu tr∆∞·ªüng", "hi·ªáu ph√≥", "c∆° s·ªü", "vi·ªán", "khoa"
        }
        return len(keys & orgish) >= 2
    except Exception:
        return False


# ========== MAIN ==========
def main():
    ap = argparse.ArgumentParser(
        description="B∆∞·ªõc 2 ‚Äî Multi-root: ƒë·ªçc 1 links.csv G·ªòP, x·ª≠ l√Ω t·ª´ng source_title v√† xu·∫•t seeds/person_edges/edu_edges + root_nodes."
    )
    ap.add_argument("--links-csv", required=True, help="links.csv ƒë√£ g·ªôp (t·ª´ B∆∞·ªõc 1 append)")
    ap.add_argument("--outdir", required=True, help="Th∆∞ m·ª•c xu·∫•t DUY NH·∫§T (kh√¥ng t·∫°o subfolder)")
    ap.add_argument("--filter-title", help="Ch·ªâ x·ª≠ l√Ω 1 root (source_title) trong file g·ªôp")
    ap.add_argument("--assume-university", action="store_true",
                    help="N·∫øu root l√† tr∆∞·ªùng, t·∫°o c·∫°nh ALUMNI_OF cho c√°c person c√≥ h·ªçc tr∆∞·ªùng ƒë√≥")
    ap.add_argument("--include-root-seed", action="store_true",
                    help="N·∫øu root l√† ng∆∞·ªùi, th√™m ch√≠nh root v√†o seeds.csv (n·∫øu trang ƒë√≥ l√† person)")
    ap.add_argument("--append", action="store_true",
                    help="B·∫≠t ch·∫ø ƒë·ªô b·ªï sung (append) v√†o c√°c file seeds/person_edges/edu_edges hi·ªán c√≥")
    ap.add_argument("--dedupe", action="store_true",
                    help="Kh·ª≠ tr√πng khi append (ƒë·ªçc file hi·ªán c√≥ ƒë·ªÉ lo·∫°i b·ªè b·∫£n ghi tr√πng)")
    ap.add_argument("--progress-every", type=int, default=50,
                    help="(fallback) In ti·∫øn ƒë·ªô m·ªói N trang khi kh√¥ng c√≥ tqdm")
    args = ap.parse_args()

    os.makedirs(args.outdir, exist_ok=True)

    # Chu·∫©n b·ªã writer (m·ªôt l·∫ßn, ghi chung)
    seeds_path   = os.path.join(args.outdir, "seeds.csv")
    p_edges_path = os.path.join(args.outdir, "person_edges.csv")
    u_edges_path = os.path.join(args.outdir, "edu_edges.csv")
    roots_path   = os.path.join(args.outdir, "root_nodes.csv")

    f_seeds,  w_seeds  = open_csv_writer(seeds_path,   ["person_title"], append=args.append)
    f_pedges, w_pedges = open_csv_writer(p_edges_path, ["src_root","dst_person","relation"], append=args.append)
    f_uedges, w_uedges = open_csv_writer(u_edges_path, ["src_university","dst_person","relation","year"], append=args.append)
    f_roots,  w_roots  = open_csv_writer(roots_path,   ["title","type"], append=args.append)

    # Dedupe sets (d√πng b·∫£n chu·∫©n ho√° ƒë·ªÉ so s√°nh)
    if args.dedupe:
        seen_seed  = load_existing_pairs_norm(seeds_path,   ["person_title"])
        seen_pedge = load_existing_pairs_norm(p_edges_path, ["src_root","dst_person","relation"])
        seen_uedge = load_existing_pairs_norm(u_edges_path, ["src_university","dst_person","relation","year"])
        seen_roots = load_existing_pairs_norm(roots_path,   ["title","type"])
    else:
        seen_seed = seen_pedge = seen_uedge = seen_roots = set()

    # Gom ·ª©ng vi√™n theo t·ª´ng source_title
    groups = load_candidates_grouped(args.links_csv, filter_title=args.filter_title)
    roots = list(groups.keys())

    total_roots = len(roots)
    print(f"üîπ T·ªïng root c·∫ßn x·ª≠ l√Ω: {total_roots}")

    for idx, root_norm in enumerate(roots, 1):
        candidates = groups[root_norm]  # list of raw target titles
        print(f"\n[{idx}/{total_roots}] ROOT = {root_norm} | candidates = {len(candidates)}")

        # L·∫•y l·∫°i ti√™u ƒë·ªÅ 'ƒë·∫πp' ƒë·ªÉ ghi ra (root_norm l√† ƒë√£ normalize)
        # ·ªû links.csv, source_title l√† normalized, n√™n root_norm ch√≠nh l√† b·∫£n chu·∫©n.
        root_title = root_norm

        # --- X√°c ƒë·ªãnh lo·∫°i trang root (person / university) ---
        root_is_person = False
        soup0 = None
        try:
            h0, _ = fetch_parse_html(root_title)
            soup0 = soup_from_html(h0)
            if soup0 and is_person_page(soup0):
                root_is_person = True
        except Exception:
            soup0 = None

        # Ghi root v√†o root_nodes.csv v·ªõi heuristic an to√†n
        if root_is_person:
            root_type = "person"
        else:
            # Heuristic: ti√™u ƒë·ªÅ + infobox
            if looks_like_university_title(root_title) or (soup0 and looks_like_university_infobox(soup0)):
                root_type = "university"
            else:
                # fallback cu·ªëi c√πng
                root_type = "university" if looks_like_university_title(root_title) else "person"

        row_root = (root_title, root_type)
        k_root = _norm_tuple(*row_root)
        if k_root not in seen_roots:
            w_roots.writerow(list(row_root))
            seen_roots.add(k_root)
            print(f"    ‚Ü≥ root_nodes: '{root_title}' ‚Üí type={root_type}")

        # N·∫øu root l√† person:
        # - th√™m ch√≠nh root v√†o seeds (depth 1 ·ªü Step 2)
        # - tr√≠ch h·ªçc v·∫•n c·ªßa root v√† ghi edu_edges (ALUMNI_OF)
        if root_is_person:
            try:
                if args.include_root_seed:
                    row_seed = (root_title,)
                    k_seed = _norm_tuple(*row_seed)
                    if k_seed not in seen_seed:
                        w_seeds.writerow([root_title]); seen_seed.add(k_seed)

                if soup0 is None:
                    # fetch l·∫°i n·∫øu c·∫ßn
                    h0, _ = fetch_parse_html(root_title)
                    soup0 = soup_from_html(h0)

                if soup0 is not None:
                    edu_root = extract_person_education(soup0) or []
                    for uni, year in edu_root:
                        row = (uni, root_title, "ALUMNI_OF", str(year) if year is not None else "")
                        k_ue = _norm_tuple(*row)
                        if k_ue not in seen_uedge:
                            w_uedges.writerow(list(row)); seen_uedge.add(k_ue)
            except Exception:
                pass

        # --- Duy·ªát c√°c candidate link c·ªßa root ---
        processed = 0
        ok_people = 0
        made_edu_edges = 0
        errors = 0

        iterator = tqdm(candidates, total=len(candidates), desc=f"Scanning@{root_title}", unit="page") if HAS_TQDM else iter(candidates)

        for cand in iterator:
            processed += 1
            try:
                ch_html, _ = fetch_parse_html(cand)
                ch_soup = soup_from_html(ch_html)
                if not ch_soup or not is_person_page(ch_soup):
                    if not HAS_TQDM and processed % args.progress_every == 0:
                        print(f"  [{processed}/{len(candidates)}] seeds={ok_people} edu={made_edu_edges} err={errors}")
                    continue

                edu = extract_person_education(ch_soup) or []
                if not edu:
                    if not HAS_TQDM and processed % args.progress_every == 0:
                        print(f"  [{processed}/{len(candidates)}] seeds={ok_people} edu={made_edu_edges} err={errors}")
                    continue

                # gi·ªØ seed (ng∆∞·ªùi) ‚Äî ƒë·ªãnh nghƒ©a: depth 1 ·ªü Step 2
                row_seed = (cand,)
                k_seed = _norm_tuple(*row_seed)
                if (not args.dedupe) or (k_seed not in seen_seed):
                    w_seeds.writerow([cand]); seen_seed.add(k_seed)
                ok_people += 1

                # c·∫°nh root -> person (LINK_FROM_START)
                row_pe = (root_title, cand, "LINK_FROM_START")
                k_pe = _norm_tuple(*row_pe)
                if (not args.dedupe) or (k_pe not in seen_pedge):
                    w_pedges.writerow(list(row_pe)); seen_pedge.add(k_pe)

                # n·∫øu ch·ªçn assume_university v√† root l√† tr∆∞·ªùng ‚Üí t·∫°o ALUMNI_OF n·∫øu ph√π h·ª£p
                if args.assume_university and (root_type == "university") and (not root_is_person):
                    added = 0
                    root_norm = normalize(root_title)
                    for uni, year in edu:
                        if normalize(uni) == root_norm:
                            row = (uni, cand, "ALUMNI_OF", str(year) if year is not None else "")
                            k_ue = _norm_tuple(*row)
                            if (not args.dedupe) or (k_ue not in seen_uedge):
                                w_uedges.writerow(list(row)); seen_uedge.add(k_ue)
                                added += 1
                    made_edu_edges += added

                if HAS_TQDM:
                    try:
                        iterator.set_postfix(seeds=ok_people, edu=made_edu_edges, err=errors)
                    except Exception:
                        pass
                else:
                    if processed % args.progress_every == 0:
                        print(f"  [{processed}/{len(candidates)}] seeds={ok_people} edu={made_edu_edges} err={errors}")

            except Exception:
                errors += 1
                if not HAS_TQDM and processed % args.progress_every == 0:
                    print(f"  [{processed}/{len(candidates)}] seeds={ok_people} edu={made_edu_edges} err={errors}")
                continue

        print(f"  ‚úÖ root done: seeds+={ok_people}, edu_edges+={made_edu_edges}, errors={errors}")

    # ƒë√≥ng file
    for fh in (f_seeds, f_pedges, f_uedges, f_roots):
        fh.close()

    print("\nüéâ Done. ƒê·∫ßu ra DUY NH·∫§T t·∫°i:", args.outdir)
    print("  - root_nodes.csv       (root + type; ph·ª•c v·ª• Step 3 set depth 0)")
    print("  - seeds.csv            (depth 1)")
    print("  - person_edges.csv     (LINK_FROM_START)")
    print("  - edu_edges.csv")


if __name__ == "__main__":
    main()
