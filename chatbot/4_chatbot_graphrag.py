# -*- coding: utf-8 -*-
"""
4_chatbot_graphrag.py
Chatbot k·∫øt h·ª£p GraphRAG + LLM Qwen OWen3 0.6B
"""
import os
import re
import unicodedata
from typing import Dict, Optional, List


class QwenLLM:
    """LLM Qwen OWen3 0.6B + GraphRAG"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2-0.5B-Instruct"):
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            print(f"\n‚è≥ Kh·ªüi t·∫°o Qwen OWen3 + GraphRAG...")
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"   Thi·∫øt b·ªã: {self.device.upper()}")
            
            print(f"   üì• Tokenizer...", end="", flush=True)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            print(f" ‚úì")
            
            print(f"   üì• Model (‚âà1.2 GB)...", end="", flush=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            print(f" ‚úì")
            print("‚úÖ Qwen OWen3 s·∫µn s√†ng v·ªõi GraphRAG!")
            self.ready = True
        except Exception as e:
            print(f"\n‚ùå L·ªói t·∫£i Qwen: {e}")
            self.ready = False
    
    def generate(self, query: str, context: str, reasoning: Optional[Dict] = None, max_tokens: int = 256, node_details_context: str = "") -> str:
        """Sinh c√¢u tr·∫£ l·ªùi t·ª´ Qwen + GraphRAG"""
        if not self.ready:
            raise RuntimeError("Qwen LLM kh√¥ng s·∫µn s√†ng. Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t.")
        
        # X√¢y d·ª±ng prompt v·ªõi GraphRAG context
        reasoning_info = ""
        if reasoning and reasoning.get('connected'):
            reasoning_info = f"T·ª´ suy lu·∫≠n ƒë·ªì th·ªã qua c√°c c·∫°nh/k·∫øt n·ªëi: {reasoning.get('description', '')}\n"
        
        # Instruction r√µ r√†ng
        instruction = "H√£y tr·∫£ l·ªùi d·ª±a tr√™n context ƒë∆∞·ª£c cung c·∫•p. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, ch·ªâ d√πng ti·∫øng Vi·ªát."
        instruction += " L∆∞u √Ω: Trong ƒë·ªì th·ªã, 'quan h·ªá' l√† c√°c c·∫°nh (edges) k·∫øt n·ªëi gi·ªØa c√°c node/th·ª±c th·ªÉ."
        if "nh·ªØng" in query.lower() or "n√†o" in query.lower() or "t·∫•t c·∫£" in query.lower():
            instruction += " Li·ªát k√™ t·∫•t c·∫£ th√¥ng tin li√™n quan."
        
        # Th√™m chi ti·∫øt node n·∫øu c√≥
        detailed_context = f"{context}"
        if node_details_context:
            detailed_context += f"\n\n=== CHI TI·∫æT TH√îNG TIN C√Å NH√ÇN ===\n{node_details_context}"
        
        prompt = f"""B·∫°n l√† chatbot th√¥ng minh v·ªÅ m·∫°ng alumni. {instruction}

CONTEXT:
{detailed_context}

{reasoning_info}

QUESTION: {query}

ANSWER:"""
        
        import torch
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            output = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=0.2,  # Gi·∫£m t·ª´ 0.7 ‚Üí 0.2 (·ªïn ƒë·ªãnh h∆°n)
                top_p=0.9,
                do_sample=True
            )
        
        response = self.tokenizer.decode(output[0], skip_special_tokens=True)
        
        # Extract answer t·ª´ response
        if "ANSWER:" in response:
            response = response.split("ANSWER:")[-1].strip()
        
        # L√†m s·∫°ch response
        response = response.strip().strip('"').strip()
        
        return response


class GraphRAGChatbot:
    """Chatbot k·∫øt h·ª£p GraphRAG + Qwen OWen3 LLM"""
    
    def __init__(self, kg, reasoner, node_details_path='../graph_out/node_details.json'):
        self.kg = kg
        self.reasoner = reasoner
        self._llm = None  # Lazy load
        
        # Load node details
        self.node_details = {}
        import json
        try:
            with open(node_details_path, 'r', encoding='utf-8') as f:
                details_list = json.load(f)
                for detail in details_list:
                    title = detail.get('title', '')
                    self.node_details[title] = detail
            print(f"   üìö ƒê√£ load {len(self.node_details)} node details")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ load node_details.json: {e}")
            self.node_details = {}
        
        print("\n" + "ü§ñ CHATBOT GRAPHRAG + QWEN OWEN3 ".center(70, "="))
        print("‚úì Knowledge Graph: ƒê·ªì th·ªã tri th·ª©c m·∫°ng alumni")
        print("‚úì Node Details: Th√¥ng tin chi ti·∫øt t·ª´ Wikipedia")
        print("‚úì GraphRAG: Truy xu·∫•t th√¥ng tin t·ª´ ƒë·ªì th·ªã")
        print("‚úì Multi-hop Reasoning: Suy lu·∫≠n k·∫øt n·ªëi ph·ª©c t·∫°p")
        print("‚úì Qwen OWen3 LLM: T·∫°o c√¢u tr·∫£ l·ªùi th√¥ng minh")
        print("=" * 70)
    
    @property
    def llm(self):
        if self._llm is None:
            self._llm = QwenLLM()
        return self._llm
    
    def answer(self, query: str) -> Dict:
        import re
        def replace_thankyou(text):
            thank_patterns = [
                r"thank you for your time and concern[.!]*",
                r"thank you[.!]*",
                r"thanks[.!]*",
                r"thank you very much[.!]*"
            ]
            for pat in thank_patterns:
                text = re.sub(pat, "C·∫£m ∆°n b·∫°n ƒë√£ quan t√¢m!", text, flags=re.IGNORECASE)
            return text

        norm_query = self._normalize_text(query)
        entities = self.reasoner._extract_entities(query)
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # B∆Ø·ªöC 1: PH√ÇN LO·∫†I C√ÇU H·ªéI NGAY T·ª™ ƒê·∫¶U (3 LO·∫†I)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        # 1. MULTIPLE CHOICE - C√¢u h·ªèi l·ª±a ch·ªçn (c√≥ A. B. C. D.)
        is_multiple_choice = bool(re.search(r'\b[A-D]\.\s*', query))
        
        # 2. YES/NO - C√¢u h·ªèi c√≥/kh√¥ng
        has_university_keyword = any(w in norm_query for w in ['tr∆∞·ªùng', 'hoc', 'h·ªçc', 'alumni', 'c√πng h·ªçc', 'c√πng tr∆∞·ªùng'])
        query_type = self._classify_query(query)
        if has_university_keyword and query_type == 'yes_no':
            query_type = 'university'
        is_yes_no = (query_type in ['yes_no', 'university', 'connection'])
        
        # 3. GENERAL - C√¢u h·ªèi c√≤n l·∫°i
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # B∆Ø·ªöC 2: X·ª¨ L√ù THEO LO·∫†I C√ÇU H·ªéI
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # LO·∫†I 1: MULTIPLE CHOICE - Ch·ªâ tr·∫£ l·ªùi ƒë√°p √°n
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if is_multiple_choice:
            return self._handle_multiple_choice(query, entities, norm_query)
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # LO·∫†I 2: YES/NO - Tr·∫£ l·ªùi C√≥/Kh√¥ng ƒë·∫ßu ti√™n
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if is_yes_no:
            return self._handle_yes_no(query, query_type, entities, norm_query, replace_thankyou)
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # LO·∫†I 3: GENERAL - C√°c c√¢u h·ªèi c√≤n l·∫°i
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        
        reasoning = None
        answer_text = None
        
        # Truy v·∫•n t·ªïng h·ª£p: l·ªçc person theo country + university (alumni)
        country_hit = self._find_node_by_type_in_query(norm_query, 'country')
        uni_hit = self._find_node_by_type_in_query(norm_query, 'university')
        aggregate_trigger = any(kw in norm_query for kw in [
            'cuu sinh vien', 'alumni', 'hoc tai', 'hoc o', 'tung hoc', 'hoc tai harvard', 'hoc tai', 'h·ªçc t·∫°i', 'c·ª±u sinh vi√™n', 'sinh vien'
        ])
        
        # Tr∆∞·ªùng h·ª£p country + university
        if aggregate_trigger and country_hit and uni_hit:
            agg = self.reasoner.find_people_by_country_and_university(country_hit, uni_hit, limit=50)
            if agg.get('missing'):
                answer_text = f"‚ùå Kh√¥ng t√¨m th·∫•y node: {', '.join(agg['missing'])}"
            elif agg['people']:
                answer_text = f"C√°c c·ª±u sinh vi√™n t·ª´ {country_hit} h·ªçc t·∫°i {uni_hit}: {', '.join(agg['people'])}"
            else:
                answer_text = f"Kh√¥ng t√¨m th·∫•y c·ª±u sinh vi√™n t·ª´ {country_hit} h·ªçc {uni_hit} trong ƒë·ªì th·ªã."
            return {
                'query': query,
                'type': 'aggregate_alumni_country_university',
                'context': '',
                'reasoning': None,
                'answer': replace_thankyou(answer_text)
            }
                # Tr∆∞·ªùng h·ª£p country + university
        if aggregate_trigger and country_hit and uni_hit:
            agg = self.reasoner.find_people_by_country_and_university(country_hit, uni_hit, limit=50)
            if agg.get('missing'):
                answer_text = f"‚ùå Kh√¥ng t√¨m th·∫•y node: {', '.join(agg['missing'])}"
            elif agg['people']:
                # ‚úÖ N·∫øu KG ƒë√£ c√≥ d·ªØ li·ªáu, d√πng lu√¥n
                answer_text = f"C√°c c·ª±u sinh vi√™n t·ª´ {country_hit} h·ªçc t·∫°i {uni_hit}: {', '.join(agg['people'])}"
            else:
                # ‚úÖ FALLBACK: d√πng node_details n·∫øu KG kh√¥ng tr·∫£ ƒë∆∞·ª£c ng∆∞·ªùi n√†o
                fallback_people = self._fallback_people_by_country_and_university(country_hit, uni_hit, limit=50)
                if fallback_people:
                    answer_text = (
                        f"C√°c c·ª±u sinh vi√™n t·ª´ {country_hit} h·ªçc t·∫°i {uni_hit} (suy ra t·ª´ node_details): "
                        f"{', '.join(sorted(set(fallback_people)))}"
                    )
                else:
                    answer_text = f"Kh√¥ng t√¨m th·∫•y c·ª±u sinh vi√™n t·ª´ {country_hit} h·ªçc {uni_hit} trong ƒë·ªì th·ªã."
            return {
                'query': query,
                'type': 'aggregate_alumni_country_university',
                'context': '',
                'reasoning': None,
                'answer': replace_thankyou(answer_text)
            }

        # Tr∆∞·ªùng h·ª£p ch·ªâ university: li·ªát k√™ c·ª±u sinh vi√™n c·ªßa tr∆∞·ªùng
        if aggregate_trigger and uni_hit and not country_hit:
            agg = self.reasoner.find_people_by_university(uni_hit, limit=100)
            if agg.get('missing'):
                answer_text = f"‚ùå Kh√¥ng t√¨m th·∫•y node: {', '.join(agg['missing'])}"
            elif agg['people']:
                answer_text = f"C√°c c·ª±u sinh vi√™n c·ªßa {uni_hit}: {', '.join(sorted(agg['people']))}"
            else:
                answer_text = f"Kh√¥ng t√¨m th·∫•y c·ª±u sinh vi√™n c·ªßa {uni_hit} trong ƒë·ªì th·ªã."
            return {
                'query': query,
                'type': 'aggregate_alumni_university',
                'context': '',
                'reasoning': None,
                'answer': replace_thankyou(answer_text)
            }

        # Nh·∫≠n di·ªán c√¢u h·ªèi li·ªát k√™ ch·ª©c v·ª•/career/country
        match = re.search(r"ph√≥ t·ªïng th·ªëng|pho_tong_thong|vice president|career|country|ch·ª©c v·ª•|position", query.lower())
        if match:
            result = []
            keywords = ['ph√≥ t·ªïng th·ªëng', 'pho_tong_thong', 'vice president', 'career', 'country', 'ch·ª©c v·ª•', 'position']
            for node_id, data in self.reasoner.kg.G.nodes(data=True):
                props = data.get('properties')
                if props and isinstance(props, dict):
                    for k, v in props.items():
                        if any(kw in str(k).lower() for kw in keywords):
                            result.append(data['title'])
                        if isinstance(v, str) and any(kw in v.lower() for kw in keywords):
                            result.append(data['title'])
                        elif isinstance(v, list) and any(any(kw in str(x).lower() for kw in keywords) for x in v):
                            result.append(data['title'])
            if result:
                answer_text = f"C√°c node li√™n quan ƒë·∫øn ch·ª©c v·ª•/country/career: {', '.join(sorted(set(result)))}"
            else:
                answer_text = "Kh√¥ng t√¨m th·∫•y node n√†o ph√π h·ª£p trong m·∫°ng l∆∞·ªõi."
            return {
                'query': query,
                'type': 'list_career_country',
                'context': '',
                'reasoning': None,
                'answer': replace_thankyou(answer_text)
            }

        # X·ª≠ l√Ω c√¢u h·ªèi v·ªÅ th√¥ng tin chi ti·∫øt m·ªôt node c·ª• th·ªÉ
        uni_hint = self._find_node_by_type_in_query(norm_query, 'university')
        if len(entities) == 1 and query_type == 'general' and not uni_hint and 'hoc' not in norm_query and 'h·ªçc' not in query.lower():
            entity_name = entities[0]
            node_detail = self.node_details.get(entity_name)
            
            # N·∫øu c√≥ th√¥ng tin chi ti·∫øt t·ª´ node_details
            if node_detail:
                info_text = self._format_node_detail(node_detail)
                return {
                    'query': query,
                    'type': 'node_detail',
                    'context': info_text[:500],
                    'reasoning': None,
                    'answer': replace_thankyou(info_text)
                }

        # Nh·∫≠n di·ªán c√¢u h·ªèi li·ªát k√™ m·ªëi quan h·ªá/c·∫°nh/k·∫øt n·ªëi
        if any(kw in query.lower() for kw in ['li·ªát k√™', 'k·ªÉ t√™n', 'c√°c m·ªëi quan h·ªá', 'nh·ªØng m·ªëi quan h·ªá', 'relationship', 'connections', 'c·∫°nh', 'k·∫øt n·ªëi']) and len(entities) >= 2:
            node1 = self.reasoner.kg.title_to_node.get(entities[0])
            node2 = self.reasoner.kg.title_to_node.get(entities[1])
            edges = []
            connected_flag = False
            # C·∫°nh thu·∫≠n (edge from node1 to node2)
            if node1 and node2 and node2 in self.reasoner.kg.G[node1]:
                rel = self.reasoner.kg.G[node1][node2]['relation']
                edges.append(f"üîó {entities[0]} --[c·∫°nh: {rel}]--> {entities[1]}")
                connected_flag = True
            # C·∫°nh ng∆∞·ª£c (edge from node2 to node1)
            if node1 and node2 and node1 in self.reasoner.kg.G[node2]:
                rel = self.reasoner.kg.G[node2][node1]['relation']
                edges.append(f"üîó {entities[1]} --[c·∫°nh: {rel}]--> {entities[0]}")
                connected_flag = True

            # Ki·ªÉm tra properties c·ªßa m·ªói node
            info1 = self.reasoner.kg.get_node_info(node1) if node1 else None
            info2 = self.reasoner.kg.get_node_info(node2) if node2 else None
            def check_properties(info_a, name_b):
                rels = []
                if info_a and info_a.get('properties'):
                    props = info_a['properties']
                    if isinstance(props, dict):
                        for k, v in props.items():
                            if isinstance(v, str) and name_b in v:
                                rels.append(f"{info_a['title']} --[{k}]--> {name_b}")
                            elif isinstance(v, list) and any(name_b in str(x) for x in v):
                                rels.append(f"{info_a['title']} --[{k}]--> {name_b}")
                return rels
            edges += check_properties(info1, entities[1])
            edges += check_properties(info2, entities[0])

            if edges:
                # lu√¥n tr·∫£ l·ªùi b·∫Øt ƒë·∫ßu b·∫±ng C√≥/Kh√¥ng
                prefix = "C√≥, "
                answer_text = prefix + f"C√°c c·∫°nh/quan h·ªá/k·∫øt n·ªëi gi·ªØa {entities[0]} v√† {entities[1]}:\n" + "\n".join(edges)
            else:
                answer_text = f"Kh√¥ng, kh√¥ng t√¨m th·∫•y c·∫°nh/quan h·ªá tr·ª±c ti·∫øp gi·ªØa {entities[0]} v√† {entities[1]} trong ƒë·ªì th·ªã.\nüí° Gi·∫£i th√≠ch: Hai th·ª±c th·ªÉ n√†y kh√¥ng c√≥ k·∫øt n·ªëi tr·ª±c ti·∫øp (c·∫°nh) trong knowledge graph."
            
            return {
                'query': query,
                'type': 'list_relationships',
                'context': '',
                'reasoning': None,
                'answer': answer_text
            }

        # N·∫øu ch∆∞a c√≥ answer, d√πng LLM
        context = self.reasoner.retrieve_context(query)
        # Th√™m node details context ƒë·ªÉ LLM c√≥ info chi ti·∫øt h∆°n
        node_details_ctx = self._build_node_details_context(entities)
        answer_text = self.llm.generate(query, context, reasoning, node_details_context=node_details_ctx)

        return {
            'query': query,
            'type': query_type,
            'context': self.reasoner.retrieve_context(query)[:300] + "...",
            'reasoning': reasoning,
            'answer': replace_thankyou(answer_text)
        }
    
    def _classify_query(self, query: str) -> str:
        """Ph√¢n lo·∫°i lo·∫°i c√¢u h·ªèi"""
        query_lower = query.lower()
        
        # Tr·∫Øc nghi·ªám l·ª±a ch·ªçn (multiple choice)
        if re.search(r'\b[A-D]\.', query_lower) or re.search(r'\b[a-d]\.', query_lower):
            return 'multiple_choice'
        
        if any(w in query_lower for w in ['k·∫øt n·ªëi', 'li√™n k·∫øt', 'quan h·ªá', 'c√≥ m·ªëi', 'ƒë∆∞·ª£c k·∫øt n·ªëi']):
            return 'connection'
        elif any(w in query_lower for w in ['tr∆∞·ªùng', 'ƒë·∫°i h·ªçc', 'c√πng tr∆∞·ªùng', 'c√πng h·ªçc', 'c√πng ƒë·∫°i h·ªçc', 'h·ªçc ', 'hoc ', 'alumni', 'h·ªçc t·∫°i', 'hoc tai']):
            return 'university'
        
        # C√¢u h·ªèi ƒê√∫ng/Sai ho·∫∑c Yes/No
        # Ch·ªâ match yes/no patterns, kh√¥ng match "c√≥ h·ªçc" ho·∫∑c "c√≥ k·∫øt n·ªëi"
        if (query_lower.startswith(('c√≥ ph·∫£i', 'c√≥ kh√°c', 'kh√¥ng ph·∫£i ', 'kh√¥ng ', 'ƒë√∫ng ', 'sai ')) or
            any(phrase in query_lower for phrase in ['ƒë√∫ng kh√¥ng', 'ph·∫£i kh√¥ng', 'sai kh√¥ng', 'c√≥ ph·∫£i', 'kh√¥ng ph·∫£i']) or
            query_lower.endswith(('kh√¥ng?', 'ph·∫£i?'))):
            return 'yes_no'
        else:
            return 'general'

    def _normalize_text(self, text: str) -> str:
        """Chu·∫©n h√≥a ƒë·ªÉ so kh·ªõp t·ª± do trong c√¢u h·ªèi"""
        import re
        s = unicodedata.normalize('NFD', text)
        s = ''.join(ch for ch in s if unicodedata.category(ch) != 'Mn')
        s = s.lower().replace('_', ' ').replace('-', ' ')
        s = re.sub(r"[^a-z0-9 ]+", " ", s)
        return " ".join(s.split())

    def _find_node_by_type_in_query(self, norm_query: str, node_type: str) -> Optional[str]:
        """T√¨m node theo lo·∫°i n·∫øu ti√™u ƒë·ªÅ xu·∫•t hi·ªán trong c√¢u h·ªèi (l·ªèng)"""
        query_tokens = set(norm_query.split())
        generic = {'country', 'dai', 'hoc', 'university', 'truong', 'o', 'dau', 'nao', 'nhung', 'sinh', 'vien', 'nguoi', 'tung', 'lam', 'tai', 'truong', 'dai', 'hoc', 'alumni', 'co', 'khong', 'ai', 'la', 'gi', 'cac'}
        best_title = None
        best_len = -1

        for _, data in self.reasoner.kg.G.nodes(data=True):
            if data.get('node_type') != node_type:
                continue
            title = data.get('title', '')
            norm_title = self._normalize_text(title)
            tokens = [t for t in norm_title.split() if t not in generic and len(t) >= 3]
            if not tokens:
                continue
            # match if all significant tokens appear in query
            if all(t in query_tokens for t in tokens):
                if len(norm_title) > best_len:
                    best_title = title
                    best_len = len(norm_title)
                continue
            # or full substring match
            if norm_title in norm_query and len(norm_title) > best_len:
                best_title = title
                best_len = len(norm_title)

        # Fallback: m·ªôt s·ªë node country c√≥ node_type 'unknown' nh∆∞ng id d·∫°ng country_*
        if node_type == 'country' and not best_title:
            for _, data in self.reasoner.kg.G.nodes(data=True):
                title = data.get('title', '')
                if not title.lower().startswith('country_'):
                    continue
                norm_title = self._normalize_text(title)
                tokens = [t for t in norm_title.split() if t not in generic and len(t) >= 3]
                if tokens and all(t in query_tokens for t in tokens):
                    if len(norm_title) > best_len:
                        best_title = title
                        best_len = len(norm_title)
                elif norm_title in norm_query and len(norm_title) > best_len:
                    best_title = title
                    best_len = len(norm_title)

        # Heuristic aliases for ph·ªï bi·∫øn
        if not best_title and node_type == 'university':
            aliases = {
                'harvard': 'ƒê·∫°i h·ªçc Harvard',
                'stanford': 'ƒê·∫°i h·ªçc Stanford',
                'mit': 'Vi·ªán C√¥ng ngh·ªá Massachusetts',
                'yale': 'ƒê·∫°i h·ªçc Yale',
                'oxford': 'ƒê·∫°i h·ªçc Oxford',
                'cambridge': 'ƒê·∫°i h·ªçc Cambridge'
            }
            for key, title in aliases.items():
                if key in query_tokens:
                    return title
        if not best_title and node_type == 'country':
            country_aliases = {
                'trung quoc': 'country_Trung_Quoc',
                'viet nam': 'country_Viet_Nam',
                'hoa ky': 'country_Hoa_Ky',
                'my': 'country_Hoa_Ky'
            }
            for key, title in country_aliases.items():
                if all(tok in query_tokens for tok in key.split()):
                    return title

        return best_title
    
    def _format_node_detail(self, node_detail: Dict) -> str:
        """ƒê·ªãnh d·∫°ng th√¥ng tin chi ti·∫øt node t·ª´ node_details.json"""
        lines = []
        
        # Title
        title = node_detail.get('title', '')
        lines.append(f"üìå {title}")
        lines.append("=" * 60)
        
        # Type
        node_type = node_detail.get('type', '')
        if node_type:
            lines.append(f"Lo·∫°i: {node_type}")
        
        # Link
        link = node_detail.get('link', '')
        if link:
            lines.append(f"Ngu·ªìn: {link}")
        
        # Properties
        properties = node_detail.get('properties', {})
        if properties and isinstance(properties, dict):
            lines.append("\nüìã Th√¥ng tin chi ti·∫øt:")
            for key, value in properties.items():
                if isinstance(value, list):
                    # N·∫øu l√† list, gh√©p th√†nh chu·ªói
                    val_str = ' '.join(str(v) for v in value if v)
                else:
                    val_str = str(value)
                lines.append(f"  ‚Ä¢ {key}: {val_str}")
        
        # Related nodes
        related = node_detail.get('related', [])
        if related:
            lines.append(f"\nüîó Ng∆∞·ªùi li√™n quan: {', '.join(related)}")
        
        return "\n".join(lines)
    
    def _build_node_details_context(self, entities: List[str], max_properties: int = 10) -> str:
        """
        X√¢y d·ª±ng context t·ª´ node_details cho LLM
        L·∫•y chi ti·∫øt t·ª´ng properties c·ªßa entities ƒë·ªÉ cung c·∫•p info to√†n di·ªán h∆°n
        """
        if not entities:
            return ""
        
        import json
        details_parts = []
        
        for entity in entities[:5]:  # Gi·ªõi h·∫°n 5 entities ƒë·ªÉ kh√¥ng qu√° d√†i
            node_detail = self.node_details.get(entity)
            if not node_detail:
                continue
            
            # Format simplified version for LLM
            entity_info = f"\nüìå {entity}:"
            
            # Type
            if node_detail.get('type'):
                entity_info += f"\n  Lo·∫°i: {node_detail.get('type')}"
            
            # Properties - l·∫•y t·ª´ng c√°i chi ti·∫øt
            properties = node_detail.get('properties', {})
            if properties and isinstance(properties, dict):
                entity_info += "\n  Th√¥ng tin:"
                for key, value in list(properties.items())[:max_properties]:
                    if isinstance(value, list):
                        val_str = ' '.join(str(v) for v in value if v)
                    else:
                        val_str = str(value)
                    # Truncate d√†i qu√°
                    if len(val_str) > 150:
                        val_str = val_str[:150] + "..."
                    entity_info += f"\n    - {key}: {val_str}"
            
            # Related people
            related = node_detail.get('related', [])
            if related:
                entity_info += f"\n  Li√™n quan ƒë·∫øn: {', '.join(related[:5])}"
            
            details_parts.append(entity_info)
        
        return "".join(details_parts) if details_parts else ""
    def _fallback_people_by_country_and_university(self, country_title: str, university_title: str, limit: int = 50):
        """
        Fallback khi ƒë·ªì th·ªã kh√¥ng ƒë·ªß c·∫°nh from_country/born_in v√† alumni_of.
        D√πng node_details.properties ƒë·ªÉ suy ra ng∆∞·ªùi thu·ªôc country + h·ªçc university.
        ∆ØU TI√äN node_details h∆°n ƒë·ªì th·ªã.
        """
        norm_country = self._normalize_text(
            country_title.replace('country_', '').replace('_', ' ')
        )
        norm_uni = self._normalize_text(university_title)

        # Alias c∆° b·∫£n cho country
        country_aliases = {
            'trung quoc': ['trung quoc', 'trung qu·ªëc', 'china', 'people s republic of china'],
            'viet nam': ['viet nam', 'vietnam'],
            'hoa ky': ['hoa ky', 'my', 'usa', 'united states', 'united states of america'],
        }
        country_keys = country_aliases.get(norm_country, [norm_country])

        people = []

        for title, detail in self.node_details.items():
            if len(people) >= limit:
                break
            if detail.get('type') != 'person':
                continue

            props = detail.get('properties', {})
            if not isinstance(props, dict):
                continue

            # Gh√©p m·ªçi value th√†nh m·ªôt chu·ªói r·ªìi chu·∫©n ho√°
            values = []
            for v in props.values():
                if isinstance(v, list):
                    values.append(' '.join(str(x) for x in v if x))
                else:
                    values.append(str(v))
            all_props_text = ' '.join(values)
            norm_props = self._normalize_text(all_props_text)

            # 1) Check country (Trung Qu·ªëc / China / ‚Ä¶)
            if not any(ck in norm_props for ck in country_keys):
                continue

            # 2) Check university (Harvard, Stanford,‚Ä¶)
            if norm_uni not in norm_props:
                continue

            people.append(title)

        return people

    def _search_by_properties(self, query: str) -> Optional[Dict]:
        """
        T√¨m ki·∫øm t·ª´ node_details d·ª±a tr√™n keywords trong properties
        H·ªó tr·ª£ c√¢u h·ªèi ph·ª©c t·∫°p nh∆∞ "Ai l√† Ph√≥ T·ªïng th·ªëng c·ªßa Abdulrahman Wahid?"
        """
        query_lower = query.lower()
        
        # Patterns ƒë·ªÉ nh·∫≠n di·ªán c√¢u h·ªèi
        property_patterns = [
            ('ph√≥ t·ªïng th·ªëng|pho tong thong|vice president', 'Ph√≥ T·ªïng th·ªëng'),
            ('t·ªïng th·ªëng|tong thong|president', 'T·ªïng th·ªëng'),
            ('sinh|born|ng√†y sinh|date of birth', 'Sinh'),
            ('m·∫•t|died|ng√†y m·∫•t|date of death', 'M·∫•t'),
            ('k·∫ø nhi·ªám|successor|ke niem', 'K·∫ø nhi·ªám'),
            ('ti·ªÅn nhi·ªám|predecessor|tien niem', 'Ti·ªÅn nhi·ªám'),
            ('ƒë·∫£ng|party|dang', 'ƒê·∫£ng ch√≠nh tr·ªã'),
            ('alma mater|tr∆∞·ªùng|h·ªçc', 'Alma mater'),
        ]
        
        for pattern, prop_key in property_patterns:
            if re.search(pattern, query_lower, re.IGNORECASE):
                # T√¨m entities trong query
                entities = self.reasoner._extract_entities(query)
                if not entities:
                    continue
                
                results = []
                for entity in entities[:3]:
                    node_detail = self.node_details.get(entity)
                    if node_detail:
                        properties = node_detail.get('properties', {})
                        if isinstance(properties, dict):
                            for key, value in properties.items():
                                # So kh·ªõp property key
                                if prop_key.lower() in key.lower():
                                    if isinstance(value, list):
                                        val_str = ' '.join(str(v) for v in value if v)
                                    else:
                                        val_str = str(value)
                                    results.append({
                                        'entity': entity,
                                        'property': key,
                                        'value': val_str
                                    })
                
                if results:
                    return {
                        'type': 'property_search',
                        'results': results,
                        'pattern': prop_key
                    }
        
        return None
    
    def _compare_alma_mater(self, entity1: str, entity2: str) -> Optional[str]:
        """
        So s√°nh alma mater c·ªßa 2 ng∆∞·ªùi t·ª´ node_details
        Tr·∫£ v·ªÅ "C√≥" n·∫øu c√πng tr∆∞·ªùng, "Kh√¥ng" n·∫øu kh√°c ho·∫∑c kh√¥ng t√¨m th·∫•y
        """
        detail1 = self.node_details.get(entity1)
        detail2 = self.node_details.get(entity2)
        
        if not detail1 or not detail2:
            return None
        
        props1 = detail1.get('properties', {})
        props2 = detail2.get('properties', {})
        
        if not isinstance(props1, dict) or not isinstance(props2, dict):
            return None
        
        # L·∫•y alma mater
        alma1 = props1.get('Alma mater', '')
        alma2 = props2.get('Alma mater', '')
        
        if not alma1 or not alma2:
            return None
        
        # Convert list to string n·∫øu c·∫ßn
        if isinstance(alma1, list):
            alma1 = ' '.join(str(v) for v in alma1 if v)
        if isinstance(alma2, list):
            alma2 = ' '.join(str(v) for v in alma2 if v)
        
        alma1_str = str(alma1).lower()
        alma2_str = str(alma2).lower()
        
        # T√¨m tr∆∞·ªùng chung
        # Extract t√™n tr∆∞·ªùng t·ª´ alma mater string
        def extract_universities(alma_str: str):
            """Extract danh s√°ch c√°c tr∆∞·ªùng t·ª´ alma mater string"""
            # Lo·∫°i b·ªè parenthesis content like "(BA)", "(JD)"
            cleaned = re.sub(r'\([^)]+\)', '', alma_str).strip()
            
            # Split b·ªüi c√°c d·∫•u ph√¢n c√°ch ph·ªï bi·∫øn
            parts = re.split(r'[;,\s{2,}]+', cleaned)  # Split by ; , ho·∫∑c nhi·ªÅu space
            
            unis = []
            for part in parts:
                part = part.strip()
                if part and ('ƒë·∫°i h·ªçc' in part.lower() or 'university' in part.lower() or 
                             'institute' in part.lower() or 'college' in part.lower()):
                    # B√¨nh th∆∞·ªùng h√≥a t√™n tr∆∞·ªùng b·∫±ng c√°ch lo·∫°i b·ªè d·∫•u
                    import unicodedata
                    normalized = unicodedata.normalize('NFD', part.lower())
                    normalized = ''.join(ch for ch in normalized if unicodedata.category(ch) != 'Mn')
                    # Lo·∫°i b·ªè "ƒë·∫°i h·ªçc" prefix ƒë·ªÉ so s√°nh ƒë∆°n gi·∫£n h∆°n
                    normalized = normalized.replace('ƒëai hoc', '').replace('university', '').strip()
                    if normalized:
                        unis.append(normalized)
            return unis
        
        unis1 = extract_universities(alma1_str)
        unis2 = extract_universities(alma2_str)
        
        # Ki·ªÉm tra c√≥ tr∆∞·ªùng chung kh√¥ng
        common = set(unis1) & set(unis2)
        
        if common:
            return "C√≥"
        else:
            return "Kh√¥ng"
    
    def _handle_multiple_choice(self, query: str, entities: List[str], norm_query: str) -> Dict:
        """X·ª≠ l√Ω c√¢u h·ªèi l·ª±a ch·ªçn (multiple choice) - ch·ªâ tr·∫£ v·ªÅ ƒë√°p √°n"""
        import re
        import unicodedata
        
        # 1. C√¢u h·ªèi v·ªÅ COUNTRY
        if 'n∆∞·ªõc n√†o' in query.lower() or ('country' in query.lower() and 'qu·ªëc gia' in query.lower()):
            if entities:
                entity = entities[0]
                node_id = self.reasoner.kg.title_to_node.get(entity)
                if node_id:
                    # T√¨m country t·ª´ edges
                    for neighbor in self.reasoner.kg.G.successors(node_id):
                        edge_data = self.reasoner.kg.G[node_id][neighbor]
                        rel = edge_data.get('relation', '')
                        if rel in ['from_country', 'born_in']:
                            country_title = self.reasoner.kg.node_to_title.get(neighbor, '')
                            country_name = country_title.replace('country_', '').replace('_', ' ')
                            
                            # Map country name to options
                            country_map = {
                                'Anh': ['Anh', 'V∆∞∆°ng qu·ªëc Anh', 'UK', 'England'],
                                'Hoa Ky': ['M·ªπ', 'Hoa K·ª≥', 'USA', 'America'],
                                'Phap': ['Ph√°p', 'France'],
                                'Duc': ['ƒê·ª©c', 'Germany'],
                                'Y': ['√ù', 'Italy']
                            }
                            
                            # Find matching option in query
                            for std_name, variants in country_map.items():
                                if std_name.lower() in country_name.lower():
                                    for variant in variants:
                                        pattern = r'([A-D])\.\s*' + re.escape(variant)
                                        match = re.search(pattern, query, re.IGNORECASE)
                                        if match:
                                            option = match.group(1)
                                            return {
                                                'query': query,
                                                'type': 'multiple_choice',
                                                'context': '',
                                                'reasoning': None,
                                                'answer': f"ƒê√°p √°n: {option}"
                                            }
        
        # 2. C√¢u h·ªèi v·ªÅ UNIVERSITY/ALMA MATER
        if any(kw in query.lower() for kw in ['ƒë·∫°i h·ªçc', 'tr∆∞·ªùng', 'university', 'alma mater', 'c·ª±u sinh vi√™n']):
            if entities:
                entity = entities[0]
                # T√¨m alma mater t·ª´ node_details
                node_detail = self.node_details.get(entity)
                if node_detail:
                    props = node_detail.get('properties', {})
                    
                    # T√¨m trong nhi·ªÅu properties c√≥ th·ªÉ ch·ª©a th√¥ng tin tr∆∞·ªùng h·ªçc
                    education_info = ''
                    for key in ['Alma mater', 'alma mater', 'Tr∆∞·ªùng l·ªõp', 'Education', 'education', 'H·ªçc v·∫•n']:
                        if key in props:
                            value = props[key]
                            if isinstance(value, list):
                                education_info += ' '.join(str(v) for v in value if v) + ' '
                            else:
                                education_info += str(value) + ' '
                    
                    if education_info:
                        education_str = education_info.lower()
                        
                        # Extract c√°c options t·ª´ query
                        options = re.findall(r'([A-D])\.\s*([^A-D]+?)(?=[A-D]\.|$)', query, re.IGNORECASE)
                        
                        # T√¨m option kh·ªõp v·ªõi education info
                        for option_letter, option_text in options:
                            option_clean = option_text.strip().lower()
                            # Normalize for comparison
                            option_normalized = unicodedata.normalize('NFD', option_clean)
                            option_normalized = ''.join(ch for ch in option_normalized if unicodedata.category(ch) != 'Mn')
                            
                            edu_normalized = unicodedata.normalize('NFD', education_str)
                            edu_normalized = ''.join(ch for ch in edu_normalized if unicodedata.category(ch) != 'Mn')
                            
                            # Check if option appears in education info
                            # T√¨m c√°c t·ª´ ch√≠nh trong option (b·ªè "ƒë·∫°i h·ªçc", "university")
                            option_keywords = [w for w in option_normalized.split() if w not in ['dai', 'hoc', 'university', 'college'] and len(w) > 3]
                            
                            if option_normalized in edu_normalized or \
                               (option_keywords and all(kw in edu_normalized for kw in option_keywords)):
                                return {
                                    'query': query,
                                    'type': 'multiple_choice',
                                    'context': '',
                                    'reasoning': None,
                                    'answer': f"ƒê√°p √°n: {option_letter}"
                                }
                
                # Fallback: t√¨m t·ª´ graph edges (alumni_of)
                node_id = self.reasoner.kg.title_to_node.get(entity)
                if node_id:
                    for neighbor in self.reasoner.kg.G.successors(node_id):
                        edge_data = self.reasoner.kg.G[node_id][neighbor]
                        rel = edge_data.get('relation', '')
                        if rel == 'alumni_of':
                            uni_title = self.reasoner.kg.node_to_title.get(neighbor, '')
                            
                            # Find matching option
                            options = re.findall(r'([A-D])\.\s*([^A-D]+?)(?=[A-D]\.|$)', query, re.IGNORECASE)
                            for option_letter, option_text in options:
                                option_clean = option_text.strip().lower()
                                uni_lower = uni_title.lower()
                                if option_clean in uni_lower or any(word in uni_lower for word in option_clean.split() if len(word) > 4):
                                    return {
                                        'query': query,
                                        'type': 'multiple_choice',
                                        'context': '',
                                        'reasoning': None,
                                        'answer': f"ƒê√°p √°n: {option_letter}"
                                    }
        
        # 3. Fallback: KH√îNG T√åM TH·∫§Y TRONG DATA
        return {
            'query': query,
            'type': 'multiple_choice',
            'context': '',
            'reasoning': None,
            'answer': "Kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c trong d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
        }
    
    def _handle_yes_no(self, query: str, query_type: str, entities: List[str], norm_query: str, replace_thankyou) -> Dict:
        """X·ª≠ l√Ω c√¢u h·ªèi Yes/No - lu√¥n b·∫Øt ƒë·∫ßu v·ªõi C√≥/Kh√¥ng"""
        import re
        
        # Ki·ªÉm tra entities c√≥ t·ªìn t·∫°i trong ƒë·ªì th·ªã kh√¥ng
        if entities:
            missing_entities = [e for e in entities if e not in self.reasoner.kg.title_to_node]
            if missing_entities and len(entities) > 0:
                if len(missing_entities) == len(entities):
                    return {
                        'query': query,
                        'type': 'entity_not_found',
                        'context': '',
                        'reasoning': None,
                        'answer': f"‚ùå Kh√¥ng t√¨m th·∫•y c√°c th·ª±c th·ªÉ sau trong ƒë·ªì th·ªã: {', '.join(missing_entities)}"
                    }
                elif query_type in ['connection', 'university']:
                    return {
                        'query': query,
                        'type': 'partial_entity_found',
                        'context': '',
                        'reasoning': None,
                        'answer': f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y c√°c th·ª±c th·ªÉ sau trong ƒë·ªì th·ªã: {', '.join(missing_entities)}"
                    }
        
        # X·ª≠ l√Ω c√¢u h·ªèi v·ªÅ m·ªôt ng∆∞·ªùi + tr∆∞·ªùng
        if query_type == 'university' and len(entities) == 1:
            person = entities[0]
            uni_hint = None
            for title in self.reasoner.kg.title_to_node.keys():
                if self.reasoner.kg.G.nodes[self.reasoner.kg.title_to_node[title]].get('node_type') == 'university':
                    norm_title = self._normalize_text(title)
                    if norm_title in norm_query:
                        uni_hint = title
                        break
            
            uni_hit = None
            if not any(kw in norm_query for kw in ['nhung', 'nao']):
                uni_hit = uni_hint or self._find_node_by_type_in_query(norm_query, 'university')
            node_id = self.reasoner.kg.title_to_node.get(person)
            
            if node_id:
                alumni = set()
                # L·∫•y t·ª´ node_details.Alma mater
                detail = self.node_details.get(person)
                if detail:
                    props = detail.get('properties', {})
                    if isinstance(props, dict):
                        alma = props.get('Alma mater') or props.get('alma mater')
                        if alma:
                            if isinstance(alma, list):
                                alma_str = ' '.join(str(v) for v in alma if v)
                            else:
                                alma_str = str(alma)
                            parts = [p.strip() for p in re.split(r'[;,]', alma_str) if p.strip()]
                            if parts:
                                alumni |= set(parts)
                
                # T·ª´ KG: alumni_of outbound
                if not alumni:
                    alumni |= {n['title'] for n in self.reasoner.kg.get_neighbors(node_id, 'alumni_of')}
                
                # N·∫øu c√¢u h·ªèi h·ªèi danh s√°ch
                if not uni_hit and any(kw in norm_query for kw in ['nhung', 'nao']):
                    if alumni:
                        answer_text = f"C√≥. {person} h·ªçc t·∫°i c√°c tr∆∞·ªùng: {', '.join(sorted(alumni))}."
                    else:
                        answer_text = f"Kh√¥ng. Kh√¥ng t√¨m th·∫•y th√¥ng tin tr∆∞·ªùng h·ªçc c·ªßa {person}."
                elif uni_hit:
                    if any(self._normalize_text(uni_hit) in self._normalize_text(a) for a in alumni):
                        answer_text = f"C√≥. {person} h·ªçc t·∫°i {uni_hit}."
                    else:
                        answer_text = f"Kh√¥ng. Kh√¥ng th·∫•y {uni_hit} trong th√¥ng tin tr∆∞·ªùng h·ªçc c·ªßa {person}."
                else:
                    if alumni:
                        answer_text = f"C√≥. {person} h·ªçc t·∫°i: {', '.join(sorted(alumni))}."
                    else:
                        answer_text = f"Kh√¥ng. Kh√¥ng t√¨m th·∫•y th√¥ng tin tr∆∞·ªùng h·ªçc c·ªßa {person}."
                
                return {
                    'query': query,
                    'type': 'university_single_person',
                    'context': '',
                    'reasoning': None,
                    'answer': replace_thankyou(answer_text)
                }
        
        # X·ª≠ l√Ω Yes/No th√¥ng th∆∞·ªùng
        if query_type == 'yes_no':
            if len(entities) >= 2:
                reasoning = self.reasoner.check_connection(entities[0], entities[1])
                if reasoning.get('missing_entities'):
                    answer_text = f"Kh√¥ng. Kh√¥ng t√¨m th·∫•y: {', '.join(reasoning['missing_entities'])}"
                elif reasoning.get('connected'):
                    answer_text = "C√≥."
                    path_desc = reasoning.get('description') or reasoning.get('explanation', '')
                    if path_desc:
                        answer_text += f" {path_desc}"
                else:
                    answer_text = "Kh√¥ng."
                    reason = reasoning.get('reason', '')
                    if reason:
                        answer_text += f" {reason}"
            else:
                context = self.reasoner.retrieve_context(query)
                prompt = f"Tr·∫£ l·ªùi C√≥/Kh√¥ng cho c√¢u h·ªèi sau: {query}\nTh√¥ng tin: {context[:300]}"
                answer_text = self.llm.generate(prompt, "", None, node_details_context="").strip()
                if answer_text.lower() not in ['c√≥', 'kh√¥ng', 'yes', 'no', 'ƒë√∫ng', 'sai']:
                    answer_text = "Kh√¥ng th·ªÉ x√°c ƒë·ªãnh"
            
            return {
                'query': query,
                'type': 'yes_no',
                'context': '',
                'reasoning': reasoning if len(entities) >= 2 else None,
                'answer': replace_thankyou(answer_text)
            }
        
        # X·ª≠ l√Ω connection
        if query_type == 'connection' and len(entities) >= 2:
            reasoning = self.reasoner.check_connection(entities[0], entities[1])
            if reasoning.get('missing_entities'):
                missing = reasoning['missing_entities']
                answer_text = f"Kh√¥ng. Kh√¥ng t√¨m th·∫•y: {', '.join(missing)}"
            elif reasoning.get('connected'):
                hops = reasoning.get('hops', 0)
                explanation = reasoning.get('explanation', '')
                answer_text = f"C√≥. {explanation}"
            else:
                reason = reasoning.get('reason', '')
                answer_text = f"Kh√¥ng. {reason}"
            
            return {
                'query': query,
                'type': 'connection',
                'context': '',
                'reasoning': reasoning,
                'answer': replace_thankyou(answer_text)
            }
        
        # X·ª≠ l√Ω university (2 ng∆∞·ªùi)
        if query_type == 'university' and len(entities) >= 2:
            alma_result = self._compare_alma_mater(entities[0], entities[1])
            if alma_result:
                if alma_result == "C√≥":
                    answer_text = f"C√≥. {entities[0]} v√† {entities[1]} c√πng h·ªçc m·ªôt tr∆∞·ªùng."
                else:
                    answer_text = f"Kh√¥ng. {entities[0]} v√† {entities[1]} kh√¥ng h·ªçc c√πng tr∆∞·ªùng."
            else:
                reasoning = self.reasoner.check_same_university(entities[0], entities[1])
                if reasoning.get('missing_entities'):
                    missing = reasoning['missing_entities']
                    answer_text = f"Kh√¥ng. Kh√¥ng t√¨m th·∫•y: {', '.join(missing)}"
                elif reasoning.get('same_university'):
                    answer_text = f"C√≥. {entities[0]} v√† {entities[1]} c√πng h·ªçc t·∫°i {reasoning['university']}."
                else:
                    answer_text = f"Kh√¥ng. {entities[0]} v√† {entities[1]} kh√¥ng h·ªçc c√πng tr∆∞·ªùng."
            
            return {
                'query': query,
                'type': 'university',
                'context': '',
                'reasoning': None,
                'answer': replace_thankyou(answer_text)
            }
        
        # Fallback
        return {
            'query': query,
            'type': query_type,
            'context': '',
            'reasoning': None,
            'answer': "Kh√¥ng th·ªÉ x√°c ƒë·ªãnh c√¢u tr·∫£ l·ªùi."
        }


if __name__ == "__main__":
    import importlib
    
    KnowledgeGraph = importlib.import_module('1_knowledge_graph').KnowledgeGraph
    GraphRAGReasoner = importlib.import_module('2_graphrag_reasoner').GraphRAGReasoner
    
    kg = KnowledgeGraph('graph_out/nodes_unified.csv', 'graph_out/edges_unified.csv')
    reasoner = GraphRAGReasoner(kg)
    chatbot = GraphRAGChatbot(kg, reasoner)
    
    # Test
    result = chatbot.answer("Barack Obama v√† Bill Clinton c√≥ k·∫øt n·ªëi kh√¥ng?")
    print(f"\n‚ùì {result['query']}")
    print(f"üí¨ {result['answer']}")
